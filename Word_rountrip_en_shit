{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GBOGEB/GBOGEB-profile/blob/main/Word_rountrip_en_shit\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, I understand. You want a more detailed expansion and revision of the previous DMAIC breakdown, including:\n",
        "\n",
        "* **Specific Code Examples:** Concrete implementations for key parts of `main.py`, helpers, and sub-functions.\n",
        "* **Enhanced Workflow and ASCII:** A more detailed ASCII representation showing the flow between the new blocks/functions/features.\n",
        "* **Updated Requirements/Rules:** Refinements to the `requires` and `rules` sections of the JSON.\n",
        "* **Fuller Repo Structure:** Elaboration on the purpose and potential content of the different directories and files.\n",
        "\n",
        "Let's proceed with this expanded and revised approach, building upon the previous information.\n",
        "\n",
        "**D - Define (Revisited):**\n",
        "\n",
        "The core goal remains enhancing the \"Schrijfeditor\" with robustness, user-friendliness, and advanced editing tools. We're now focusing on providing concrete implementations and clearer visual representations.\n",
        "\n",
        "**M - Measure (Revisited):**\n",
        "\n",
        "The metrics for success still revolve around improved user experience, reduced errors, and increased efficiency. The key is to see how the implemented code and workflow directly contribute to these improvements.\n",
        "\n",
        "**A - Analyze (Revisited):**\n",
        "\n",
        "The analysis still points to the need for better path management, automated checks, helpful utilities, and robust error handling. The opportunity lies in translating these needs into functional code and a clear workflow.\n",
        "\n",
        "**I - Improve (Expanded):**\n",
        "\n",
        "Let's delve into the specific code and workflow enhancements.\n",
        "\n",
        "**1. Updated `cli.py` Code with Examples:**"
      ],
      "metadata": {
        "id": "TQvy5RuBTmGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is A_file"
      ],
      "metadata": {
        "id": "ymPp_9-qYL4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the input directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(\"input\", exist_ok=True)\n",
        "\n",
        "# Create a dummy input file\n",
        "dummy_content = \"\"\"# My Document\n",
        "\n",
        "## Introduction\n",
        "This is a sample document.\n",
        "\n",
        "## Requirements\n",
        "Requirement 1: The system shall provide a user interface.\n",
        "Requirement 2: The system shall be measurable.\n",
        "\n",
        "## Glossary\n",
        "Term: API - Application Programming Interface\n",
        "Term: UI - User Interface\n",
        "\"\"\"\n",
        "\n",
        "input_filename = \"input_document.md\"\n",
        "input_path = os.path.join(\"input\", input_filename)\n",
        "\n",
        "with open(input_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(dummy_content)\n",
        "\n",
        "print(f\"Created dummy input file at {input_path}\")"
      ],
      "metadata": {
        "id": "j8pzxrsdZXqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import datetime\n",
        "import subprocess\n",
        "import json\n",
        "\n",
        "# --- Configuration ---\n",
        "CONFIG_FILE = \"config.json\"\n",
        "DEFAULT_INPUT_FOLDER = \"input\"\n",
        "DEFAULT_OUTPUT_FOLDER = \"output\"\n",
        "DEFAULT_BACKUP_INPUT_ONEDRIVE = os.path.expanduser(\"~/OneDrive/Backup/Input\")\n",
        "DEFAULT_BACKUP_OUTPUT_ONEDRIVE = os.path.expanduser(\"~/OneDrive/Backup/Output\")\n",
        "\n",
        "class PathsManager:\n",
        "    def __init__(self, config_file=CONFIG_FILE):\n",
        "        self.config = self._load_config(config_file)\n",
        "        self.input_folder = self.config.get(\"input_folder\", DEFAULT_INPUT_FOLDER)\n",
        "        self.output_folder = self.config.get(\"output_folder\", DEFAULT_OUTPUT_FOLDER)\n",
        "        self.backup_input_onedrive = self.config.get(\"backup_input_onedrive\", DEFAULT_BACKUP_INPUT_ONEDRIVE)\n",
        "        self.backup_output_onedrive = self.config.get(\"backup_output_onedrive\", DEFAULT_BACKUP_OUTPUT_ONEDRIVE)\n",
        "        self._ensure_paths_exist()\n",
        "\n",
        "    def _load_config(self, config_file):\n",
        "        if os.path.exists(config_file):\n",
        "            try:\n",
        "                with open(config_file, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Warning: Could not decode config file '{config_file}'. Using defaults.\")\n",
        "                return {}\n",
        "        return {}\n",
        "\n",
        "    def _ensure_paths_exist(self):\n",
        "        os.makedirs(self.input_folder, exist_ok=True)\n",
        "        os.makedirs(self.output_folder, exist_ok=True)\n",
        "        # Note: We don't automatically create OneDrive backup paths\n",
        "\n",
        "    def get_input_path(self, filename=\"\"):\n",
        "        return os.path.join(self.input_folder, filename)\n",
        "\n",
        "    def get_output_path(self, filename=\"\"):\n",
        "        return os.path.join(self.output_folder, filename)\n",
        "\n",
        "    def get_backup_input_path(self, folder_name=\"input\"):\n",
        "        return os.path.join(self.backup_input_onedrive, folder_name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
        "\n",
        "    def get_backup_output_path(self, folder_name=\"output\"):\n",
        "        return os.path.join(self.backup_output_onedrive, folder_name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
        "\n",
        "paths_manager = PathsManager()\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name: str):\n",
        "        self.name = name\n",
        "        self.context: Dict = {}\n",
        "\n",
        "    def set_context(self, context: Dict):\n",
        "        self.context = context\n",
        "\n",
        "    def process(self, input_data: Any) -> Dict:\n",
        "        raise NotImplementedError\n",
        "\n",
        "class EditorCore(Agent):\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        doc_length = len(document_content.split())\n",
        "        doc_type = \"Technical Requirements Document (SoR)\"\n",
        "        feedback = f\"Initial assessment: Document length approx. {doc_length} words. Identified as '{doc_type}'.\\nConfirm?\"\n",
        "        self.context.update({\"document_type\": doc_type, \"doc_length\": doc_length})\n",
        "        return {\"output_data\": document_content, \"feedback\": feedback, \"hold_point\": True, \"prompt_for_user\": \"Confirm document type and goals.\"}\n",
        "\n",
        "class StructuredDataLoader(Agent):\n",
        "    def process(self, structured_data_content: str) -> Dict:\n",
        "        try:\n",
        "            parsed_data = json.loads(structured_data_content) # Example: Assuming JSON\n",
        "            self.context.update({\"structured_requirements\": parsed_data, \"structured_data_valid\": True})\n",
        "            feedback = \"Structured data loaded successfully.\"\n",
        "        except json.JSONDecodeError as e:\n",
        "            self.context.update({\"structured_data_valid\": False})\n",
        "            feedback = f\"Error loading structured data: {e}\"\n",
        "        return {\"output_data\": structured_data_content, \"feedback\": feedback, \"hold_point\": True, \"prompt_for_user\": \"Review parsed structured data.\"}\n",
        "\n",
        "class StyleGuideProcessor(Agent):\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        # Load from data/style_guide.json (example)\n",
        "        style_rules = {}\n",
        "        style_guide_path = paths_manager.get_input_path(\"style_guide.json\") # Assuming in input for simplicity\n",
        "        if os.path.exists(style_guide_path):\n",
        "            try:\n",
        "                with open(style_guide_path, 'r') as f:\n",
        "                    style_rules = json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                print(\"Warning: Could not load style guide.\")\n",
        "        inferred_rules = self.context.get(\"style_rules\", style_rules) # Prioritize loaded\n",
        "        self.context.update({\"style_rules\": inferred_rules})\n",
        "        feedback = f\"Style rules loaded/inferred: {inferred_rules}\"\n",
        "        return {\"output_data\": document_content, \"feedback\": feedback, \"hold_point\": True, \"prompt_for_user\": \"Review and confirm style rules.\"}\n",
        "\n",
        "class StructuralAnalyzer(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        section_type = \"unknown\"\n",
        "        if section_content.startswith(\"## Requirements\"):\n",
        "            section_type = \"Requirements Section\"\n",
        "        elif section_content.startswith(\"## Glossary\"):\n",
        "            section_type = \"Glossary Section\"\n",
        "        self.context.update({\"current_section_type\": section_type})\n",
        "        feedback = f\"Analyzed section structure: Type - {section_type}\"\n",
        "        return {\"output_data\": section_content, \"feedback\": feedback, \"hold_point\": False}\n",
        "\n",
        "def check_grammar(text, rules=None):\n",
        "    errors = []\n",
        "    if \"shall provides\" in text:\n",
        "        errors.append({\"original\": \"shall provides\", \"suggested\": \"shall provide\", \"explanation\": \"Grammar: verb agreement.\"})\n",
        "    # ... more sophisticated checks based on rules ...\n",
        "    return errors\n",
        "\n",
        "class GrammarEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        edits = []\n",
        "        lines = section_content.splitlines()\n",
        "        style_rules = self.context.get(\"style_rules\", {})\n",
        "        for i, line in enumerate(lines):\n",
        "            grammar_errors = check_grammar(line, style_rules.get(\"grammar_checks\"))\n",
        "            for error in grammar_errors:\n",
        "                edits.append({\"line\": i + 1, **error})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Grammar suggestions.\", \"suggested_edits\": edits}\n",
        "\n",
        "def check_spelling(text, dictionary=None):\n",
        "    errors = []\n",
        "    if \"teh\" in text:\n",
        "        errors.append({\"original\": \"teh\", \"suggested\": \"the\", \"explanation\": \"Spelling error.\"})\n",
        "    # ... more sophisticated checks using dictionary ...\n",
        "    return errors\n",
        "\n",
        "class SpellingEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        edits = []\n",
        "        lines = section_content.splitlines()\n",
        "        # Load dictionary from paths_manager.get_input_path(\"spelling_dict.txt\") (example)\n",
        "        for i, line in enumerate(lines):\n",
        "            spelling_errors = check_spelling(line, self.context.get(\"spelling_dictionary\"))\n",
        "            for error in spelling_errors:\n",
        "                edits.append({\"line\": i + 1, **error})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Spelling suggestions.\", \"suggested_edits\": edits}\n",
        "\n",
        "class ConsistencyEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        edits = []\n",
        "        style_rules = self.context.get(\"style_rules\", {})\n",
        "        terminology = style_rules.get(\"terminology_consistency\", {})\n",
        "        for original, preferred in terminology.items():\n",
        "            if original in section_content:\n",
        "                # Basic replacement - needs more sophisticated handling\n",
        "                section_content = section_content.replace(original, preferred)\n",
        "                edits.append({\"original\": original, \"suggested\": preferred, \"explanation\": f\"Consistent use of '{preferred}'.\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Consistency checks.\", \"suggested_edits\": edits}\n",
        "\n",
        "class ClarityEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        suggestions = []\n",
        "        if \"very important\" in section_content:\n",
        "            suggestions.append({\"line\": section_content.find(\"very important\"), \"suggestion\": \"Consider using a more precise term than 'very important'.\", \"type\": \"Clarity\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Clarity suggestions.\", \"clarity_suggestions\": suggestions}\n",
        "\n",
        "class RequirementValidator(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        issues = []\n",
        "        if self.context.get(\"structured_requirements\"):\n",
        "            for req_id, details in self.context[\"structured_requirements\"].items():\n",
        "                if req_id in section_content and \"measurability\" not in section_content.lower():\n",
        "                    issues.append({\"requirement\": req_id, \"issue\": \"Measurability not explicitly mentioned.\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Requirement validation.\", \"validation_issues\": issues}\n",
        "\n",
        "class ContextReviewer(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        suggestions = []\n",
        "        if self.context.get(\"current_section_type\") == \"Requirements Section\" and \"background information\" in section_content.lower():\n",
        "            suggestions.append({\"suggestion\": \"Consider moving background information to a dedicated section.\", \"type\": \"Structure\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Context review.\", \"context_suggestions\": suggestions}\n",
        "\n",
        "class FeedbackAggregator(Agent):\n",
        "    def process(self, section_content: str, all_agent_outputs: List[Dict]) -> Dict:\n",
        "        aggregated_feedback = {\"edits\": [], \"issues\": [], \"suggestions\": [], \"violations\": []}\n",
        "        for output in all_agent_outputs:\n",
        "            aggregated_feedback[\"edits\"].extend(output.get(\"suggested_edits\", []))\n",
        "            aggregated_feedback[\"issues\"].extend(output.get(\"validation_issues\", []))\n",
        "            aggregated_feedback[\"suggestions\"].extend(output.get(\"clarity_suggestions\", []) + output.get(\"context_suggestions\", []))\n",
        "            aggregated_feedback[\"violations\"].extend(output.get(\"violations\", []))\n",
        "\n",
        "        formatted_output = \"### Aggregated Feedback for Section:\\n\"\n",
        "        if aggregated_feedback[\"edits\"]:\n",
        "            formatted_output += \"**Proposed Edits:**\\n\" + \"\\n\".join([f\"- Line {e['line']}: `{e['original']}` -> `{e['suggested']}` ({e['explanation']})\" for e in aggregated_feedback[\"edits\"]])\n",
        "        if aggregated_feedback[\"issues\"]:\n",
        "            formatted_output += \"\\n**Validation Issues:**\\n\" + \"\\n\".join([f\"- {i['requirement']}: {i['issue']}\" for i in aggregated_feedback[\"issues\"]])\n",
        "        if aggregated_feedback[\"suggestions\"]:\n",
        "            formatted_output += \"\\n**Suggestions:**\\n\" + \"\\n\".join([f\"- {s['type']}: {s['suggestion']}\" for s in aggregated_feedback[\"suggestions\"]])\n",
        "        if aggregated_feedback[\"violations\"]:\n",
        "            formatted_output += \"\\n**Style Violations:**\\n\" + \"\\n\".join([f\"- {v}\" for v in aggregated_feedback[\"violations\"]])\n",
        "\n",
        "        return {\"output_data\": section_content, \"feedback\": formatted_output, \"hold_point\": True, \"prompt_for_user\": \"Review aggregated feedback. Accept/Reject/Revise?\"}\n",
        "\n",
        "class FinalComposer(Agent):\n",
        "    def process(self, full_edited_document: str) -> Dict:\n",
        "        # Basic Markdown formatting\n",
        "        final_md = \"# Edited Document\\n\\n\" + full_edited_document\n",
        "        report = f\"# Editing Report\\n\\nGenerated on: {datetime.datetime.now()}\"\n",
        "        return {\"output_data\": final_md, \"feedback\": \"Final document and report generated.\", \"final_document_md\": final_md, \"editing_report_md\": report, \"hold_point\": True, \"prompt_for_user\": \"Review final document and report.\"}\n",
        "\n",
        "class DiagnosticsAgent(Agent):\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        report = []\n",
        "        if \"TODO:\" in document_content:\n",
        "            report.append(\"Warning: Contains 'TODO:' markers.\")\n",
        "        # Example: Check for consistent heading styles (very basic)\n",
        "        headings = [line for line in document_content.splitlines() if line.startswith(\"#\")]\n",
        "        if headings and not all(h.startswith(\"## \") for h in headings):\n",
        "            report.append(\"Suggestion: Consider consistent use of second-level headings (##).\")\n",
        "        return {\"output_data\": document_content, \"feedback\": \"Diagnostics report:\", \"report\": \"\\n\".join(report), \"hold_point\": True, \"prompt_for_user\": \"Review diagnostics report.\"}\n",
        "\n",
        "class HelpersAgent(Agent):\n",
        "    def extract_glossary(self, document_content: str) -> List[str]:\n",
        "        glossary = [line.split(\":\")[1].strip() for line in document_content.splitlines() if line.startswith(\"Term:\")]\n",
        "        return glossary\n",
        "\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        helpers_output = {}\n",
        "        if \"extract glossary\" in input(\"Available helpers: extract glossary. Which helper? \").lower():\n",
        "            helpers_output[\"glossary\"] = self.extract_glossary(document_content)\n",
        "            feedback = f\"Extracted glossary: {helpers_output['glossary']}\"\n",
        "            hold_point = True\n",
        "            prompt_for_user = \"Review extracted glossary.\"\n",
        "        else:\n",
        "            feedback = \"No helper function invoked.\"\n",
        "            hold_point = False\n",
        "            prompt_for_user = None\n",
        "        return {\"output_data\": document_content, \"feedback\": feedback, \"helpers_output\": helpers_output, \"hold_point\": hold_point, \"prompt_for_user\": prompt_for_user}\n",
        "\n",
        "class FixerAgent(Agent):\n",
        "    def process(self, section_content: str, suggested_edits: List[Dict]) -> Dict:\n",
        "        fixed_content = section_content\n",
        "        applied_fixes = []\n",
        "        for edit in suggested_edits:\n",
        "            if edit[\"explanation\"].startswith(\"Spelling:\") and input(f\"Auto-fix '{edit['original']}' to '{edit['suggested']}'? (y/n)\").lower() == 'y':\n",
        "                fixed_content = fixed_content.replace(edit[\"original\"], edit[\"suggested\"], 1)\n",
        "                applied_fixes.append(edit)\n",
        "        return {\"output_data\": fixed_content, \"feedback\": f\"Applied automatic fixes: {applied_fixes}\", \"applied_fixes\": applied_fixes}\n",
        "\n",
        "class CheckerAgent(Agent):\n",
        "    def process(self, section_content: str, style_rules: Dict) -> Dict:\n",
        "        violations = []\n",
        "        if style_rules.get(\"heading_capitalization\") == \"title case\":\n",
        "            for line in section_content.splitlines():\n",
        "                if line.startswith(\"#\") and line != line.title():\n",
        "                    violations.append(f\"Warning: Heading '{line.strip()}' is not title-cased.\")\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Style checks performed.\", \"violations\": violations, \"hold_point\": True, \"prompt_for_user\": \"Review style check violations.\"}\n",
        "\n",
        "class AnalyzerAgent(Agent):\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        readability = len(document_content.split()) / (document_content.count('.') + 1 if document_content.count('.') > 0 else 1) # Very basic\n",
        "        return {\"output_data\": document_content, \"feedback\": f\"Readability: {readability:.2f}\", \"readability_score\": readability, \"hold_point\": False}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting document processing pipeline...\")\n",
        "\n",
        "    # Initialize agents\n",
        "    editor_core = EditorCore(\"EditorCore\")\n",
        "    structured_data_loader = StructuredDataLoader(\"StructuredDataLoader\")\n",
        "    style_guide_processor = StyleGuideProcessor(\"StyleGuideProcessor\")\n",
        "    structural_analyzer = StructuralAnalyzer(\"StructuralAnalyzer\")\n",
        "    grammar_editor = GrammarEditor(\"GrammarEditor\")\n",
        "    spelling_editor = SpellingEditor(\"SpellingEditor\")\n",
        "    consistency_editor = ConsistencyEditor(\"ConsistencyEditor\")\n",
        "    clarity_editor = ClarityEditor(\"ClarityEditor\")\n",
        "    requirement_validator = RequirementValidator(\"RequirementValidator\")\n",
        "    context_reviewer = ContextReviewer(\"ContextReviewer\")\n",
        "    feedback_aggregator = FeedbackAggregator(\"FeedbackAggregator\")\n",
        "    final_composer = FinalComposer(\"FinalComposer\")\n",
        "    diagnostics_agent = DiagnosticsAgent(\"DiagnosticsAgent\")\n",
        "    helpers_agent = HelpersAgent(\"HelpersAgent\")\n",
        "    fixer_agent = FixerAgent(\"FixerAgent\")\n",
        "    checker_agent = CheckerAgent(\"CheckerAgent\")\n",
        "    analyzer_agent = AnalyzerAgent(\"AnalyzerAgent\")\n",
        "\n",
        "\n",
        "    agents = [\n",
        "        editor_core,\n",
        "        structured_data_loader,\n",
        "        style_guide_processor,\n",
        "        structural_analyzer,\n",
        "        grammar_editor,\n",
        "        spelling_editor,\n",
        "        consistency_editor,\n",
        "        clarity_editor,\n",
        "        requirement_validator,\n",
        "        context_reviewer,\n",
        "        feedback_aggregator,\n",
        "        final_composer,\n",
        "        diagnostics_agent,\n",
        "        helpers_agent,\n",
        "        fixer_agent,\n",
        "        checker_agent,\n",
        "        analyzer_agent,\n",
        "\n",
        "    ]\n",
        "\n",
        "    shared_context = {}\n",
        "    current_document_content = \"\"\n",
        "\n",
        "    # Load input document\n",
        "    input_filename = \"input_document.md\" # Example filename\n",
        "    input_path = paths_manager.get_input_path(input_filename)\n",
        "\n",
        "    if not os.path.exists(input_path):\n",
        "        print(f\"Error: Input file not found at {input_path}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        with open(input_path, 'r', encoding='utf-8') as f:\n",
        "            current_document_content = f.read()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading input file: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"Loaded document: {input_filename}\")\n",
        "\n",
        "    # --- Pipeline Execution ---\n",
        "    processed_content = current_document_content\n",
        "    sectioned_content = processed_content.split(\"\\n## \") # Very basic sectioning\n",
        "    if not sectioned_content[0].startswith(\"#\"):\n",
        "        sectioned_content[0] = \"## \" + sectioned_content[0] # Ensure first part is treated as a section\n",
        "\n",
        "    full_edited_document_sections = []\n",
        "\n",
        "    for i, section in enumerate(sectioned_content):\n",
        "        if i > 0:\n",
        "            section = \"## \" + section # Add back the section marker lost in split\n",
        "\n",
        "        print(f\"\\n--- Processing Section {i+1} ---\")\n",
        "        current_section_content = section\n",
        "        section_agent_outputs = []\n",
        "\n",
        "        # Non-section-specific agents process the whole document or context first\n",
        "        if i == 0: # Process document-level agents once\n",
        "            print(\"Running document-level agents...\")\n",
        "            for agent in [editor_core, structured_data_loader, style_guide_processor, diagnostics_agent, analyzer_agent]:\n",
        "                agent.set_context(shared_context)\n",
        "                output = agent.process(current_document_content)\n",
        "                shared_context.update(agent.context) # Update shared context with agent's context\n",
        "                section_agent_outputs.append(output)\n",
        "                print(f\"- {agent.name}: {output.get('feedback', 'Processed')}\")\n",
        "                if output.get(\"hold_point\"):\n",
        "                    print(f\"HOLD POINT: {output.get('prompt_for_user', 'User interaction required.')}\")\n",
        "                    input(\"Press Enter to continue...\") # Simple blocking hold\n",
        "\n",
        "            # Load structured data and style guide if they exist\n",
        "            structured_data_path = paths_manager.get_input_path(\"structured_requirements.json\")\n",
        "            if os.path.exists(structured_data_path):\n",
        "                 try:\n",
        "                     with open(structured_data_path, 'r') as f:\n",
        "                         shared_context[\"structured_requirements\"] = json.load(f)\n",
        "                     print(\"Loaded structured requirements.\")\n",
        "                 except json.JSONDecodeError:\n",
        "                     print(\"Warning: Could not load structured requirements JSON.\")\n",
        "\n",
        "            style_guide_path = paths_manager.get_input_path(\"style_guide.json\")\n",
        "            if os.path.exists(style_guide_path):\n",
        "                try:\n",
        "                    with open(style_guide_path, 'r') as f:\n",
        "                        shared_context[\"style_rules\"] = json.load(f)\n",
        "                    print(\"Loaded style guide rules.\")\n",
        "                except json.JSONDecodeError:\n",
        "                    print(\"Warning: Could not load style guide JSON.\")\n",
        "\n",
        "\n",
        "          #Process section-specific agents\n",
        "        section_agents = [\n",
        "            structural_analyzer,\n",
        "            grammar_editor,\n",
        "            spelling_editor,\n",
        "\n",
        "\n",
        "\n",
        "            consistency_editor,\n",
        "            clarity_editor,\n",
        "            requirement_validator,\n",
        "            context_reviewer,\n",
        "            checker_agent, # Style checker per section\n",
        "        ]\n",
        "\n",
        "        for agent in section_agents:\n",
        "            agent.set_context(shared_context)\n",
        "            # Check if the agent is CheckerAgent and pass style_rules\n",
        "            if isinstance(agent, CheckerAgent):\n",
        "                output = agent.process(current_section_content, shared_context.get(\"style_rules\", {})) # Pass style_rules\n",
        "            else:\n",
        "                output = agent.process(current_section_content)\n",
        "\n",
        "            shared_context.update(agent.context)\n",
        "            section_agent_outputs.append(output)\n",
        "            print(f\"- {agent.name}: {output.get('feedback', 'Processed')}\")\n",
        "            if output.get(\"hold_point\"):\n",
        "                print(f\"HOLD POINT: {output.get('prompt_for_user', 'User interaction required.')}\")\n",
        "                input(\"Press Enter to continue...\")\n",
        "\n",
        "\n",
        "\n",
        "        # Aggregation and Fixer (could be iterative)\n",
        "        print(\"\\nRunning Feedback Aggregator...\")\n",
        "        feedback_aggregator.set_context(shared_context)\n",
        "        aggregated_feedback_output = feedback_aggregator.process(current_section_content, section_agent_outputs)\n",
        "        section_agent_outputs.append(aggregated_feedback_output)\n",
        "        print(f\"- {feedback_aggregator.name}: {aggregated_feedback_output.get('feedback', 'Aggregated')}\")\n",
        "        if aggregated_feedback_output.get(\"hold_point\"):\n",
        "            print(f\"HOLD POINT: {aggregated_feedback_output.get('prompt_for_user', 'User interaction required.')}\")\n",
        "            user_action = input(\"Enter 'accept' to apply all edits, 'fix' to run Fixer: \").lower()\n",
        "\n",
        "            if user_action == 'fix':\n",
        "                print(\"\\nRunning Fixer Agent...\")\n",
        "                fixer_agent.set_context(shared_context)\n",
        "                fixer_output = fixer_agent.process(current_section_content, aggregated_feedback_output.get(\"edits\", []))\n",
        "                current_section_content = fixer_output[\"output_data\"]\n",
        "                print(f\"- {fixer_agent.name}: {fixer_output.get('feedback', 'Fixed')}\")\n",
        "            elif user_action == 'accept':\n",
        "                 # Simple apply all suggested edits (needs more sophisticated implementation)\n",
        "                 for edit in aggregated_feedback_output.get(\"edits\", []):\n",
        "                     # This is a simplistic replace; a real implementation needs line-by-line precision\n",
        "                     current_section_content = current_section_content.replace(edit['original'], edit['suggested'], 1)\n",
        "                 print(\"Applied all suggested edits.\")\n",
        "\n",
        "\n",
        "        full_edited_document_sections.append(current_section_content)\n",
        "\n",
        "    # Reconstruct the full document\n",
        "    full_edited_document = \"\\n\".join(full_edited_document_sections)\n",
        "\n",
        "    # Final Composition\n",
        "    print(\"\\nRunning Final Composer...\")\n",
        "    final_composer.set_context(shared_context)\n",
        "    final_output = final_composer.process(full_edited_document)\n",
        "    print(f\"- {final_composer.name}: {final_output.get('feedback', 'Composed')}\")\n",
        "    if final_output.get(\"hold_point\"):\n",
        "        print(f\"HOLD POINT: {final_output.get('prompt_for_user', 'User interaction required.')}\")\n",
        "        input(\"Press Enter to generate output files...\")\n",
        "\n",
        "    # Save output files\n",
        "    output_document_path = paths_manager.get_output_path(\"edited_document.md\")\n",
        "    editing_report_path = paths_manager.get_output_path(\"editing_report.md\")\n",
        "\n",
        "    try:\n",
        "        with open(output_document_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(final_output.get(\"final_document_md\", full_edited_document))\n",
        "        print(f\"Edited document saved to {output_document_path}\")\n",
        "\n",
        "        with open(editing_report_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(final_output.get(\"editing_report_md\", \"No report generated.\"))\n",
        "        print(f\"Editing report saved to {editing_report_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing output files: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Backup input and output folders (example using shutil)\n",
        "    backup_input_path = paths_manager.get_backup_input_path()\n",
        "    backup_output_path = paths_manager.get_backup_output_path()\n",
        "\n",
        "    try:\n",
        "        shutil.copytree(paths_manager.input_folder, backup_input_path)\n",
        "        print(f\"Backed up input folder to {backup_input_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not backup input folder - {e}\")\n",
        "\n",
        "    try:\n",
        "        shutil.copytree(paths_manager.output_folder, backup_output_path)\n",
        "        print(f\"Backed up output folder to {backup_output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not backup output folder - {e}\")\n",
        "\n",
        "    print(\"\\nProcessing complete.\")\n",
        "import datetime\n",
        "import subprocess\n",
        "import json\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "# --- Configuration ---\n",
        "CONFIG_FILE = \"config.json\"\n",
        "DEFAULT_INPUT_FOLDER = \"input\"\n",
        "DEFAULT_OUTPUT_FOLDER = \"output\"\n",
        "DEFAULT_BACKUP_INPUT_ONEDRIVE = os.path.expanduser(\"~/OneDrive/Backup/Input\")\n",
        "DEFAULT_BACKUP_OUTPUT_ONEDRIVE = os.path.expanduser(\"~/OneDrive/Backup/Output\")\n",
        "\n",
        "class PathsManager:\n",
        "    def __init__(self, config_file=CONFIG_FILE):\n",
        "        self.config = self._load_config(config_file)\n",
        "        self.input_folder = self.config.get(\"input_folder\", DEFAULT_INPUT_FOLDER)\n",
        "        self.output_folder = self.config.get(\"output_folder\", DEFAULT_OUTPUT_FOLDER)\n",
        "        self.backup_input_onedrive = self.config.get(\"backup_input_onedrive\", DEFAULT_BACKUP_INPUT_ONEDRIVE)\n",
        "        self.backup_output_onedrive = self.config.get(\"backup_output_onedrive\", DEFAULT_BACKUP_OUTPUT_ONEDRIVE)\n",
        "        self._ensure_paths_exist()\n",
        "\n",
        "    def _load_config(self, config_file):\n",
        "        if os.path.exists(config_file):\n",
        "            try:\n",
        "                with open(config_file, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Warning: Could not decode config file '{config_file}'. Using defaults.\")\n",
        "                return {}\n",
        "        return {}\n",
        "\n",
        "    def _ensure_paths_exist(self):\n",
        "        os.makedirs(self.input_folder, exist_ok=True)\n",
        "        os.makedirs(self.output_folder, exist_ok=True)\n",
        "        # Note: We don't automatically create OneDrive backup paths\n",
        "\n",
        "    def get_input_path(self, filename=\"\"):\n",
        "        return os.path.join(self.input_folder, filename)\n",
        "\n",
        "    def get_output_path(self, filename=\"\"):\n",
        "        return os.path.join(self.output_folder, filename)\n",
        "\n",
        "    def get_backup_input_path(self, folder_name=\"input\"):\n",
        "        return os.path.join(self.backup_input_onedrive, folder_name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
        "\n",
        "    def get_backup_output_path(self, folder_name=\"output\"):\n",
        "        return os.path.join(self.backup_output_onedrive, folder_name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
        "\n",
        "paths_manager = PathsManager()\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name: str):\n",
        "        self.name = name\n",
        "        self.context: Dict = {}\n",
        "\n",
        "    def set_context(self, context: Dict):\n",
        "        self.context = context\n",
        "\n",
        "    def process(self, input_data: Any) -> Dict:\n",
        "        raise NotImplementedError\n",
        "\n",
        "class EditorCore(Agent):\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        doc_length = len(document_content.split())\n",
        "        doc_type = \"Technical Requirements Document (SoR)\"\n",
        "        feedback = f\"Initial assessment: Document length approx. {doc_length} words. Identified as '{doc_type}'.\\nConfirm?\"\n",
        "        self.context.update({\"document_type\": doc_type, \"doc_length\": doc_length})\n",
        "        return {\"output_data\": document_content, \"feedback\": feedback, \"hold_point\": True, \"prompt_for_user\": \"Confirm document type and goals.\"}\n",
        "\n",
        "class StructuredDataLoader(Agent):\n",
        "    def process(self, structured_data_content: str) -> Dict:\n",
        "        try:\n",
        "            parsed_data = json.loads(structured_data_content) # Example: Assuming JSON\n",
        "            self.context.update({\"structured_requirements\": parsed_data, \"structured_data_valid\": True})\n",
        "            feedback = \"Structured data loaded successfully.\"\n",
        "        except json.JSONDecodeError as e:\n",
        "            self.context.update({\"structured_data_valid\": False})\n",
        "            feedback = f\"Error loading structured data: {e}\"\n",
        "        return {\"output_data\": structured_data_content, \"feedback\": feedback, \"hold_point\": True, \"prompt_for_user\": \"Review parsed structured data.\"}\n",
        "\n",
        "class StyleGuideProcessor(Agent):\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        # Load from data/style_guide.json (example)\n",
        "        style_rules = {}\n",
        "        style_guide_path = paths_manager.get_input_path(\"style_guide.json\") # Assuming in input for simplicity\n",
        "        if os.path.exists(style_guide_path):\n",
        "            try:\n",
        "                with open(style_guide_path, 'r') as f:\n",
        "                    style_rules = json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                print(\"Warning: Could not load style guide.\")\n",
        "        inferred_rules = self.context.get(\"style_rules\", style_rules) # Prioritize loaded\n",
        "        self.context.update({\"style_rules\": inferred_rules})\n",
        "        feedback = f\"Style rules loaded/inferred: {inferred_rules}\"\n",
        "        return {\"output_data\": document_content, \"feedback\": feedback, \"hold_point\": True, \"prompt_for_user\": \"Review and confirm style rules.\"}\n",
        "\n",
        "class StructuralAnalyzer(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        section_type = \"unknown\"\n",
        "        if section_content.startswith(\"## Requirements\"):\n",
        "            section_type = \"Requirements Section\"\n",
        "        elif section_content.startswith(\"## Glossary\"):\n",
        "            section_type = \"Glossary Section\"\n",
        "        self.context.update({\"current_section_type\": section_type})\n",
        "        feedback = f\"Analyzed section structure: Type - {section_type}\"\n",
        "        return {\"output_data\": section_content, \"feedback\": feedback, \"hold_point\": False}\n",
        "\n",
        "def check_grammar(text, rules=None):\n",
        "    errors = []\n",
        "    if \"shall provides\" in text:\n",
        "        errors.append({\"original\": \"shall provides\", \"suggested\": \"shall provide\", \"explanation\": \"Grammar: verb agreement.\"})\n",
        "    # ... more sophisticated checks based on rules ...\n",
        "    return errors\n",
        "\n",
        "class GrammarEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        edits = []\n",
        "        lines = section_content.splitlines()\n",
        "        style_rules = self.context.get(\"style_rules\", {})\n",
        "        for i, line in enumerate(lines):\n",
        "            grammar_errors = check_grammar(line, style_rules.get(\"grammar_checks\"))\n",
        "            for error in grammar_errors:\n",
        "                edits.append({\"line\": i + 1, **error})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Grammar suggestions.\", \"suggested_edits\": edits}\n",
        "\n",
        "def check_spelling(text, dictionary=None):\n",
        "    errors = []\n",
        "    if \"teh\" in text:\n",
        "        errors.append({\"original\": \"teh\", \"suggested\": \"the\", \"explanation\": \"Spelling error.\"})\n",
        "    # ... more sophisticated checks using dictionary ...\n",
        "    return errors\n",
        "\n",
        "class SpellingEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        edits = []\n",
        "        lines = section_content.splitlines()\n",
        "        # Load dictionary from paths_manager.get_input_path(\"spelling_dict.txt\") (example)\n",
        "        for i, line in enumerate(lines):\n",
        "            spelling_errors = check_spelling(line, self.context.get(\"spelling_dictionary\"))\n",
        "            for error in spelling_errors:\n",
        "                edits.append({\"line\": i + 1, **error})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Spelling suggestions.\", \"suggested_edits\": edits}\n",
        "\n",
        "class ConsistencyEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        edits = []\n",
        "        style_rules = self.context.get(\"style_rules\", {})\n",
        "        terminology = style_rules.get(\"terminology_consistency\", {})\n",
        "        for original, preferred in terminology.items():\n",
        "            if original in section_content:\n",
        "                # Basic replacement - needs more sophisticated handling\n",
        "                section_content = section_content.replace(original, preferred)\n",
        "                edits.append({\"original\": original, \"suggested\": preferred, \"explanation\": f\"Consistent use of '{preferred}'.\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Consistency checks.\", \"suggested_edits\": edits}\n",
        "\n",
        "class ClarityEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        suggestions = []\n",
        "        if \"very important\" in section_content:\n",
        "            suggestions.append({\"line\": section_content.find(\"very important\"), \"suggestion\": \"Consider using a more precise term than 'very important'.\", \"type\": \"Clarity\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Clarity suggestions.\", \"clarity_suggestions\": suggestions}\n",
        "\n",
        "class RequirementValidator(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        issues = []\n",
        "        if self.context.get(\"structured_requirements\"):\n",
        "            for req_id, details in self.context[\"structured_requirements\"].items():\n",
        "                if req_id in section_content and \"measurability\" not in section_content.lower():\n",
        "                    issues.append({\"requirement\": req_id, \"issue\": \"Measurability not explicitly mentioned.\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Requirement validation.\", \"validation_issues\": issues}\n",
        "\n",
        "class ContextReviewer(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        suggestions = []\n",
        "        if self.context.get(\"current_section_type\") == \"Requirements Section\" and \"background information\" in section_content.lower():\n",
        "            suggestions.append({\"suggestion\": \"Consider moving background information to a dedicated section.\", \"type\": \"Structure\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Context review.\", \"context_suggestions\": suggestions}\n",
        "\n",
        "class FeedbackAggregator(Agent):\n",
        "    def process(self, section_content: str, all_agent_outputs: List[Dict]) -> Dict:\n",
        "        aggregated_feedback = {\"edits\": [], \"issues\": [], \"suggestions\": [], \"violations\": []}\n",
        "        for output in all_agent_outputs:\n",
        "            aggregated_feedback[\"edits\"].extend(output.get(\"suggested_edits\", []))\n",
        "            aggregated_feedback[\"issues\"].extend(output.get(\"validation_issues\", []))\n",
        "            aggregated_feedback[\"suggestions\"].extend(output.get(\"clarity_suggestions\", []) + output.get(\"context_suggestions\", []))\n",
        "            aggregated_feedback[\"violations\"].extend(output.get(\"violations\", []))\n",
        "\n",
        "        formatted_output = \"### Aggregated Feedback for Section:\\n\"\n",
        "        if aggregated_feedback[\"edits\"]:\n",
        "            formatted_output += \"**Proposed Edits:**\\n\" + \"\\n\".join([f\"- Line {e['line']}: `{e['original']}` -> `{e['suggested']}` ({e['explanation']})\" for e in aggregated_feedback[\"edits\"]])\n",
        "        if aggregated_feedback[\"issues\"]:\n",
        "            formatted_output += \"\\n**Validation Issues:**\\n\" + \"\\n\".join([f\"- {i['requirement']}: {i['issue']}\" for i in aggregated_feedback[\"issues\"]])\n",
        "        if aggregated_feedback[\"suggestions\"]:\n",
        "            formatted_output += \"\\n**Suggestions:**\\n\" + \"\\n\".join([f\"- {s['type']}: {s['suggestion']}\" for s in aggregated_feedback[\"suggestions\"]])\n",
        "        if aggregated_feedback[\"violations\"]:\n",
        "            formatted_output += \"\\n**Style Violations:**\\n\" + \"\\n\".join([f\"- {v}\" for v in aggregated_feedback[\"violations\"]])\n",
        "\n",
        "        return {\"output_data\": section_content, \"feedback\": formatted_output, \"hold_point\": True, \"prompt_for_user\": \"Review aggregated feedback. Accept/Reject/Revise?\"}\n",
        "\n",
        "class FinalComposer(Agent):\n",
        "    def process(self, full_edited_document: str) -> Dict:\n",
        "        # Basic Markdown formatting\n",
        "        final_md = \"# Edited Document\\n\\n\" + full_edited_document\n",
        "        report = f\"# Editing Report\\n\\nGenerated on: {datetime.datetime.now()}\"\n",
        "        return {\"output_data\": final_md, \"feedback\": \"Final document and report generated.\", \"final_document_md\": final_md, \"editing_report_md\": report, \"hold_point\": True, \"prompt_for_user\": \"Review final document and report.\"}\n",
        "\n",
        "class DiagnosticsAgent(Agent):\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        report = []\n",
        "        if \"TODO:\" in document_content:\n",
        "            report.append(\"Warning: Contains 'TODO:' markers.\")\n",
        "        # Example: Check for consistent heading styles (very basic)\n",
        "        headings = [line for line in document_content.splitlines() if line.startswith(\"#\")]\n",
        "        if headings and not all(h.startswith(\"## \") for h in headings):\n",
        "            report.append(\"Suggestion: Consider consistent use of second-level headings (##).\")\n",
        "        return {\"output_data\": document_content, \"feedback\": \"Diagnostics report:\", \"report\": \"\\n\".join(report), \"hold_point\": True, \"prompt_for_user\": \"Review diagnostics report.\"}\n",
        "\n",
        "class HelpersAgent(Agent):\n",
        "    def extract_glossary(self, document_content: str) -> List[str]:\n",
        "        glossary = [line.split(\":\")[1].strip() for line in document_content.splitlines() if line.startswith(\"Term:\")]\n",
        "        return glossary\n",
        "\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        helpers_output = {}\n",
        "        user_input = input(\"Available helpers: extract glossary. Which helper? \").lower()\n",
        "        if \"extract glossary\" in user_input:\n",
        "            helpers_output[\"glossary\"] = self.extract_glossary(document_content)\n",
        "            feedback = f\"Extracted glossary: {helpers_output['glossary']}\"\n",
        "            hold_point = True\n",
        "            prompt_for_user = \"Review extracted glossary.\"\n",
        "        else:\n",
        "            feedback = \"No helper function invoked.\"\n",
        "            hold_point = False\n",
        "            prompt_for_user = None\n",
        "        return {\"output_data\": document_content, \"feedback\": feedback, \"helpers_output\": helpers_output, \"hold_point\": hold_point, \"prompt_for_user\": prompt_for_user}\n",
        "\n",
        "class FixerAgent(Agent):\n",
        "    def process(self, section_content: str, suggested_edits: List[Dict]) -> Dict:\n",
        "        fixed_content = section_content\n",
        "        applied_fixes = []\n",
        "        for edit in suggested_edits:\n",
        "            if edit[\"explanation\"].startswith(\"Spelling:\") and input(f\"Auto-fix '{edit['original']}' to '{edit['suggested']}'? (y/n)\").lower() == 'y':\n",
        "                fixed_content = fixed_content.replace(edit[\"original\"], edit[\"suggested\"], 1)\n",
        "                applied_fixes.append(edit)\n",
        "        return {\"output_data\": fixed_content, \"feedback\": f\"Applied automatic fixes: {applied_fixes}\", \"applied_fixes\": applied_fixes}\n",
        "\n",
        "class CheckerAgent(Agent):\n",
        "    def process(self, section_content: str, style_rules: Dict) -> Dict:\n",
        "        violations = []\n",
        "        if style_rules.get(\"heading_capitalization\") == \"title case\":\n",
        "            for line in section_content.splitlines():\n",
        "                if line.startswith(\"#\") and line != line.title():\n",
        "                    violations.append(f\"Warning: Heading '{line.strip()}' is not title-cased.\")\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Style checks performed.\", \"violations\": violations, \"hold_point\": True, \"prompt_for_user\": \"Review style check violations.\"}\n",
        "\n",
        "class AnalyzerAgent(Agent):\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        readability = len(document_content.split()) / (document_content.count('.') + 1 if document_content.count('.') > 0 else 1) # Very basic\n",
        "        return {\"output_data\": document_content, \"feedback\": f\"Readability: {readability:.2f}\", \"readability_score\": readability, \"hold_point\": False}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting document processing pipeline...\")\n",
        "\n",
        "    # Initialize agents\n",
        "    editor_core = EditorCore(\"EditorCore\")\n",
        "    structured_data_loader = StructuredDataLoader(\"StructuredDataLoader\")\n",
        "    style_guide_processor = StyleGuideProcessor(\"StyleGuideProcessor\")\n",
        "    structural_analyzer = StructuralAnalyzer(\"StructuralAnalyzer\")\n",
        "    grammar_editor = GrammarEditor(\"GrammarEditor\")\n",
        "    spelling_editor = SpellingEditor(\"SpellingEditor\")\n",
        "    consistency_editor = ConsistencyEditor(\"ConsistencyEditor\")\n",
        "    clarity_editor = ClarityEditor(\"ClarityEditor\")\n",
        "    requirement_validator = RequirementValidator(\"RequirementValidator\")\n",
        "    context_reviewer = ContextReviewer(\"ContextReviewer\")\n",
        "    feedback_aggregator = FeedbackAggregator(\"FeedbackAggregator\")\n",
        "    final_composer = FinalComposer(\"FinalComposer\")\n",
        "    diagnostics_agent = DiagnosticsAgent(\"DiagnosticsAgent\")\n",
        "    helpers_agent = HelpersAgent(\"HelpersAgent\")\n",
        "    fixer_agent = FixerAgent(\"FixerAgent\")\n",
        "    checker_agent = CheckerAgent(\"CheckerAgent\")\n",
        "    analyzer_agent = AnalyzerAgent(\"AnalyzerAgent\")\n",
        "\n",
        "\n",
        "    agents = [\n",
        "        editor_core,\n",
        "        structured_data_loader,\n",
        "        style_guide_processor,\n",
        "        structural_analyzer,\n",
        "        grammar_editor,\n",
        "        spelling_editor,\n",
        "        consistency_editor,\n",
        "        clarity_editor,\n",
        "        requirement_validator,\n",
        "        context_reviewer,\n",
        "        feedback_aggregator,\n",
        "        final_composer,\n",
        "        diagnostics_agent,\n",
        "        helpers_agent,\n",
        "        fixer_agent,\n",
        "        checker_agent,\n",
        "        analyzer_agent,\n",
        "\n",
        "    ]\n",
        "\n",
        "    shared_context = {}\n",
        "    current_document_content = \"\"\n",
        "\n",
        "    # Load input document\n",
        "    input_filename = \"input_document.md\" # Example filename\n",
        "    input_path = paths_manager.get_input_path(input_filename)\n",
        "\n",
        "    if not os.path.exists(input_path):\n",
        "        print(f\"Error: Input file not found at {input_path}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        with open(input_path, 'r', encoding='utf-8') as f:\n",
        "            current_document_content = f.read()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading input file: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"Loaded document: {input_filename}\")\n",
        "\n",
        "    # --- Pipeline Execution ---\n",
        "    processed_content = current_document_content\n",
        "    sectioned_content = processed_content.split(\"\\n## \") # Very basic sectioning\n",
        "    if not sectioned_content[0].startswith(\"#\"):\n",
        "        sectioned_content[0] = \"## \" + sectioned_content[0] # Ensure first part is treated as a section\n",
        "\n",
        "    full_edited_document_sections = []\n",
        "\n",
        "    for i, section in enumerate(sectioned_content):\n",
        "        if i > 0:\n",
        "            section = \"## \" + section # Add back the section marker lost in split\n",
        "\n",
        "        print(f\"\\n--- Processing Section {i+1} ---\")\n",
        "        current_section_content = section\n",
        "        section_agent_outputs = []\n",
        "\n",
        "        # Non-section-specific agents process the whole document or context first\n",
        "        if i == 0: # Process document-level agents once\n",
        "            print(\"Running document-level agents...\")\n",
        "            for agent in [editor_core, structured_data_loader, style_guide_processor, diagnostics_agent, analyzer_agent]:\n",
        "                agent.set_context(shared_context)\n",
        "                output = agent.process(current_document_content)\n",
        "                shared_context.update(agent.context) # Update shared context with agent's context\n",
        "                section_agent_outputs.append(output)\n",
        "                print(f\"- {agent.name}: {output.get('feedback', 'Processed')}\")\n",
        "                if output.get(\"hold_point\"):\n",
        "                    print(f\"HOLD POINT: {output.get('prompt_for_user', 'User interaction required.')}\")\n",
        "                    input(\"Press Enter to continue...\") # Simple blocking hold\n",
        "\n",
        "            # Load structured data and style guide if they exist\n",
        "            structured_data_path = paths_manager.get_input_path(\"structured_requirements.json\")\n",
        "            if os.path.exists(structured_data_path):\n",
        "                 try:\n",
        "                     with open(structured_data_path, 'r') as f:\n",
        "                         shared_context[\"structured_requirements\"] = json.load(f)\n",
        "                     print(\"Loaded structured requirements.\")\n",
        "                 except json.JSONDecodeError:\n",
        "                     print(\"Warning: Could not load structured requirements JSON.\")\n",
        "\n",
        "            style_guide_path = paths_manager.get_input_path(\"style_guide.json\")\n",
        "            if os.path.exists(style_guide_path):\n",
        "                try:\n",
        "                    with open(style_guide_path, 'r') as f:\n",
        "                        shared_context[\"style_rules\"] = json.load(f)\n",
        "                    print(\"Loaded style guide rules.\")\n",
        "                except json.JSONDecodeError:\n",
        "                    print(\"Warning: Could not load style guide JSON.\")\n",
        "\n",
        "\n",
        "        # Process section-specific agents\n",
        "        section_agents = [\n",
        "            structural_analyzer,\n",
        "            grammar_editor,\n",
        "            spelling_editor,\n",
        "            consistency_editor,\n",
        "            clarity_editor,\n",
        "            requirement_validator,\n",
        "            context_reviewer,\n",
        "            checker_agent, # Style checker per section\n",
        "        ]\n",
        "\n",
        "        for agent in section_agents:\n",
        "            agent.set_context(shared_context)\n",
        "            output = agent.process(current_section_content)\n",
        "            shared_context.update(agent.context)\n",
        "            section_agent_outputs.append(output)\n",
        "            print(f\"- {agent.name}: {output.get('feedback', 'Processed')}\")\n",
        "            if output.get(\"hold_point\"):\n",
        "                print(f\"HOLD POINT: {output.get('prompt_for_user', 'User interaction required.')}\")\n",
        "                input(\"Press Enter to continue...\")\n",
        "\n",
        "\n",
        "        # Aggregation and Fixer (could be iterative)\n",
        "        print(\"\\nRunning Feedback Aggregator...\")\n",
        "        feedback_aggregator.set_context(shared_context)\n",
        "        aggregated_feedback_output = feedback_aggregator.process(current_section_content, section_agent_outputs)\n",
        "        section_agent_outputs.append(aggregated_feedback_output)\n",
        "        print(f\"- {feedback_aggregator.name}: {aggregated_feedback_output.get('feedback', 'Aggregated')}\")\n",
        "        if aggregated_feedback_output.get(\"hold_point\"):\n",
        "            print(f\"HOLD POINT: {aggregated_feedback_output.get('prompt_for_user', 'User interaction required.')}\")\n",
        "            user_action = input(\"Enter 'accept' to apply all edits, 'fix' to run Fixer: \").lower()\n",
        "\n",
        "            if user_action == 'fix':\n",
        "                print(\"\\nRunning Fixer Agent...\")\n",
        "                fixer_agent.set_context(shared_context)\n",
        "                fixer_output = fixer_agent.process(current_section_content, aggregated_feedback_output.get(\"edits\", []))\n",
        "                current_section_content = fixer_output[\"output_data\"]\n",
        "                print(f\"- {fixer_agent.name}: {fixer_output.get('feedback', 'Fixed')}\")\n",
        "            elif user_action == 'accept':\n",
        "                 # Simple apply all suggested edits (needs more sophisticated implementation)\n",
        "                 for edit in aggregated_feedback_output.get(\"edits\", []):\n",
        "                     # This is a simplistic replace; a real implementation needs line-by-line precision\n",
        "                     current_section_content = current_section_content.replace(edit['original'], edit['suggested'], 1)\n",
        "                 print(\"Applied all suggested edits.\")\n",
        "\n",
        "\n",
        "        full_edited_document_sections.append(current_section_content)\n",
        "\n",
        "    # Reconstruct the full document\n",
        "    full_edited_document = \"\\n\".join(full_edited_document_sections)\n",
        "\n",
        "    # Final Composition\n",
        "    print(\"\\nRunning Final Composer...\")\n",
        "    final_composer.set_context(shared_context)\n",
        "    final_output = final_composer.process(full_edited_document)\n",
        "    print(f\"- {final_composer.name}: {final_output.get('feedback', 'Composed')}\")\n",
        "    if final_output.get(\"hold_point\"):\n",
        "        print(f\"HOLD POINT: {final_output.get('prompt_for_user', 'User interaction required.')}\")\n",
        "        input(\"Press Enter to generate output files...\")\n",
        "\n",
        "    # Save output files\n",
        "    output_document_path = paths_manager.get_output_path(\"edited_document.md\")\n",
        "    editing_report_path = paths_manager.get_output_path(\"editing_report.md\")\n",
        "\n",
        "    try:\n",
        "        with open(output_document_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(final_output.get(\"final_document_md\", full_edited_document))\n",
        "        print(f\"Edited document saved to {output_document_path}\")\n",
        "\n",
        "        with open(editing_report_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(final_output.get(\"editing_report_md\", \"No report generated.\"))\n",
        "        print(f\"Editing report saved to {editing_report_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing output files: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Backup input and output folders (example using shutil)\n",
        "    backup_input_path = paths_manager.get_backup_input_path()\n",
        "    backup_output_path = paths_manager.get_backup_output_path()\n",
        "\n",
        "    try:\n",
        "        shutil.copytree(paths_manager.input_folder, backup_input_path)\n",
        "        print(f\"Backed up input folder to {backup_input_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not backup input folder - {e}\")\n",
        "\n",
        "    try:\n",
        "        shutil.copytree(paths_manager.output_folder, backup_output_path)\n",
        "        print(f\"Backed up output folder to {backup_output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not backup output folder - {e}\")\n",
        "\n",
        "    print(\"\\nProcessing complete.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting document processing pipeline...\n",
            "Loaded document: input_document.md\n",
            "\n",
            "--- Processing Section 1 ---\n",
            "Running document-level agents...\n",
            "- EditorCore: Initial assessment: Document length approx. 41 words. Identified as 'Technical Requirements Document (SoR)'.\n",
            "Confirm?\n",
            "HOLD POINT: Confirm document type and goals.\n",
            "Press Enter to continue...\n",
            "- StructuredDataLoader: Error loading structured data: Expecting value: line 1 column 1 (char 0)\n",
            "HOLD POINT: Review parsed structured data.\n",
            "Press Enter to continue...\n",
            "- StyleGuideProcessor: Style rules loaded/inferred: {}\n",
            "HOLD POINT: Review and confirm style rules.\n",
            "Press Enter to continue...\n",
            "- DiagnosticsAgent: Diagnostics report:\n",
            "HOLD POINT: Review diagnostics report.\n",
            "Press Enter to continue...\n",
            "- AnalyzerAgent: Readability: 10.25\n",
            "- StructuralAnalyzer: Analyzed section structure: Type - unknown\n",
            "- GrammarEditor: Grammar suggestions.\n",
            "- SpellingEditor: Spelling suggestions.\n",
            "- ConsistencyEditor: Consistency checks.\n",
            "- ClarityEditor: Clarity suggestions.\n",
            "- RequirementValidator: Requirement validation.\n",
            "- ContextReviewer: Context review.\n",
            "- CheckerAgent: Style checks performed.\n",
            "HOLD POINT: Review style check violations.\n",
            "Press Enter to continue...\n",
            "\n",
            "Running Feedback Aggregator...\n",
            "- FeedbackAggregator: ### Aggregated Feedback for Section:\n",
            "\n",
            "HOLD POINT: Review aggregated feedback. Accept/Reject/Revise?\n",
            "Enter 'accept' to apply all edits, 'fix' to run Fixer: \n",
            "\n",
            "--- Processing Section 2 ---\n",
            "- StructuralAnalyzer: Analyzed section structure: Type - unknown\n",
            "- GrammarEditor: Grammar suggestions.\n",
            "- SpellingEditor: Spelling suggestions.\n",
            "- ConsistencyEditor: Consistency checks.\n",
            "- ClarityEditor: Clarity suggestions.\n",
            "- RequirementValidator: Requirement validation.\n",
            "- ContextReviewer: Context review.\n",
            "- CheckerAgent: Style checks performed.\n",
            "HOLD POINT: Review style check violations.\n",
            "Press Enter to continue...\n",
            "\n",
            "Running Feedback Aggregator...\n",
            "- FeedbackAggregator: ### Aggregated Feedback for Section:\n",
            "\n",
            "HOLD POINT: Review aggregated feedback. Accept/Reject/Revise?\n",
            "Enter 'accept' to apply all edits, 'fix' to run Fixer: \n",
            "\n",
            "--- Processing Section 3 ---\n",
            "- StructuralAnalyzer: Analyzed section structure: Type - Requirements Section\n",
            "- GrammarEditor: Grammar suggestions.\n",
            "- SpellingEditor: Spelling suggestions.\n",
            "- ConsistencyEditor: Consistency checks.\n",
            "- ClarityEditor: Clarity suggestions.\n",
            "- RequirementValidator: Requirement validation.\n",
            "- ContextReviewer: Context review.\n",
            "- CheckerAgent: Style checks performed.\n",
            "HOLD POINT: Review style check violations.\n",
            "Press Enter to continue...\n",
            "\n",
            "Running Feedback Aggregator...\n",
            "- FeedbackAggregator: ### Aggregated Feedback for Section:\n",
            "\n",
            "HOLD POINT: Review aggregated feedback. Accept/Reject/Revise?\n",
            "Enter 'accept' to apply all edits, 'fix' to run Fixer: \n",
            "\n",
            "--- Processing Section 4 ---\n",
            "- StructuralAnalyzer: Analyzed section structure: Type - Glossary Section\n",
            "- GrammarEditor: Grammar suggestions.\n",
            "- SpellingEditor: Spelling suggestions.\n",
            "- ConsistencyEditor: Consistency checks.\n",
            "- ClarityEditor: Clarity suggestions.\n",
            "- RequirementValidator: Requirement validation.\n",
            "- ContextReviewer: Context review.\n",
            "- CheckerAgent: Style checks performed.\n",
            "HOLD POINT: Review style check violations.\n",
            "Press Enter to continue...\n",
            "\n",
            "Running Feedback Aggregator...\n",
            "- FeedbackAggregator: ### Aggregated Feedback for Section:\n",
            "\n",
            "HOLD POINT: Review aggregated feedback. Accept/Reject/Revise?\n",
            "Enter 'accept' to apply all edits, 'fix' to run Fixer: \n",
            "\n",
            "Running Final Composer...\n",
            "- FinalComposer: Final document and report generated.\n",
            "HOLD POINT: Review final document and report.\n",
            "Press Enter to generate output files...\n",
            "Edited document saved to output/edited_document.md\n",
            "Editing report saved to output/editing_report.md\n",
            "Backed up input folder to /root/OneDrive/Backup/Input/input_20250606_132718\n",
            "Backed up output folder to /root/OneDrive/Backup/Output/output_20250606_132718\n",
            "\n",
            "Processing complete.\n",
            "Starting document processing pipeline...\n",
            "Loaded document: input_document.md\n",
            "\n",
            "--- Processing Section 1 ---\n",
            "Running document-level agents...\n",
            "- EditorCore: Initial assessment: Document length approx. 41 words. Identified as 'Technical Requirements Document (SoR)'.\n",
            "Confirm?\n",
            "HOLD POINT: Confirm document type and goals.\n",
            "Press Enter to continue...\n",
            "- StructuredDataLoader: Error loading structured data: Expecting value: line 1 column 1 (char 0)\n",
            "HOLD POINT: Review parsed structured data.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjyTjE8fTmGq",
        "outputId": "26414737-62e5-4825-9597-ec8981dddecc"
      }
    },
    {
      "source": [
        "\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import datetime\n",
        "import subprocess\n",
        "import json\n",
        "from typing import Dict, List, Any # Import Dict, List, and Any from the typing module\n",
        "\n",
        "# --- Configuration ---\n",
        "CONFIG_FILE = \"config.json\"\n",
        "DEFAULT_INPUT_FOLDER = \"input\"\n",
        "DEFAULT_OUTPUT_FOLDER = \"output\"\n",
        "DEFAULT_BACKUP_INPUT_ONEDRIVE = os.path.expanduser(\"~/OneDrive/Backup/Input\")\n",
        "DEFAULT_BACKUP_OUTPUT_ONEDRIVE = os.path.expanduser(\"~/OneDrive/Backup/Output\")\n",
        "\n",
        "class PathsManager:\n",
        "    def __init__(self, config_file=CONFIG_FILE):\n",
        "        self.config = self._load_config(config_file)\n",
        "        self.input_folder = self.config.get(\"input_folder\", DEFAULT_INPUT_FOLDER)\n",
        "        self.output_folder = self.config.get(\"output_folder\", DEFAULT_OUTPUT_FOLDER)\n",
        "        self.backup_input_onedrive = self.config.get(\"backup_input_onedrive\", DEFAULT_BACKUP_INPUT_ONEDRIVE)\n",
        "        self.backup_output_onedrive = self.config.get(\"backup_output_onedrive\", DEFAULT_BACKUP_OUTPUT_ONEDRIVE)\n",
        "        self._ensure_paths_exist()\n",
        "\n",
        "    def _load_config(self, config_file):\n",
        "        if os.path.exists(config_file):\n",
        "            try:\n",
        "                with open(config_file, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Warning: Could not decode config file '{config_file}'. Using defaults.\")\n",
        "                return {}\n",
        "        return {}\n",
        "\n",
        "    def _ensure_paths_exist(self):\n",
        "        os.makedirs(self.input_folder, exist_ok=True)\n",
        "        os.makedirs(self.output_folder, exist_ok=True)\n",
        "        # Note: We don't automatically create OneDrive backup paths\n",
        "\n",
        "    def get_input_path(self, filename=\"\"):\n",
        "        return os.path.join(self.input_folder, filename)\n",
        "\n",
        "    def get_output_path(self, filename=\"\"):\n",
        "        return os.path.join(self.output_folder, filename)\n",
        "\n",
        "    def get_backup_input_path(self, folder_name=\"input\"):\n",
        "        return os.path.join(self.backup_input_onedrive, folder_name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
        "\n",
        "    def get_backup_output_path(self, folder_name=\"output\"):\n",
        "        return os.path.join(self.backup_output_onedrive, folder_name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
        "\n",
        "paths_manager = PathsManager()\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name: str):\n",
        "        self.name = name\n",
        "        self.context: Dict = {}\n",
        "\n",
        "    def set_context(self, context: Dict):\n",
        "        self.context = context\n",
        "\n",
        "    def process(self, input_data: Any) -> Dict:\n",
        "        raise NotImplementedError\n",
        "\n",
        "class EditorCore(Agent):\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        doc_length = len(document_content.split())\n",
        "        doc_type = \"Technical Requirements Document (SoR)\"\n",
        "        feedback = f\"Initial assessment: Document length approx. {doc_length} words. Identified as '{doc_type}'.\\nConfirm?\"\n",
        "        self.context.update({\"document_type\": doc_type, \"doc_length\": doc_length})\n",
        "        return {\"output_data\": document_content, \"feedback\": feedback, \"hold_point\": True, \"prompt_for_user\": \"Confirm document type and goals.\"}\n",
        "\n",
        "class StructuredDataLoader(Agent):\n",
        "    def process(self, structured_data_content: str) -> Dict:\n",
        "        try:\n",
        "            parsed_data = json.loads(structured_data_content) # Example: Assuming JSON\n",
        "            self.context.update({\"structured_requirements\": parsed_data, \"structured_data_valid\": True})\n",
        "            feedback = \"Structured data loaded successfully.\"\n",
        "        except json.JSONDecodeError as e:\n",
        "            self.context.update({\"structured_data_valid\": False})\n",
        "            feedback = f\"Error loading structured data: {e}\"\n",
        "        return {\"output_data\": structured_data_content, \"feedback\": feedback, \"hold_point\": True, \"prompt_for_user\": \"Review parsed structured data.\"}\n",
        "\n",
        "class StyleGuideProcessor(Agent):\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        # Load from data/style_guide.json (example)\n",
        "        style_rules = {}\n",
        "        style_guide_path = paths_manager.get_input_path(\"style_guide.json\") # Assuming in input for simplicity\n",
        "        if os.path.exists(style_guide_path):\n",
        "            try:\n",
        "                with open(style_guide_path, 'r') as f:\n",
        "                    style_rules = json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                print(\"Warning: Could not load style guide.\")\n",
        "        inferred_rules = self.context.get(\"style_rules\", style_rules) # Prioritize loaded\n",
        "        self.context.update({\"style_rules\": inferred_rules})\n",
        "        feedback = f\"Style rules loaded/inferred: {inferred_rules}\"\n",
        "        return {\"output_data\": document_content, \"feedback\": feedback, \"hold_point\": True, \"prompt_for_user\": \"Review and confirm style rules.\"}\n",
        "\n",
        "class StructuralAnalyzer(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        section_type = \"unknown\"\n",
        "        if section_content.startswith(\"## Requirements\"):\n",
        "            section_type = \"Requirements Section\"\n",
        "        elif section_content.startswith(\"## Glossary\"):\n",
        "            section_type = \"Glossary Section\"\n",
        "        self.context.update({\"current_section_type\": section_type})\n",
        "        feedback = f\"Analyzed section structure: Type - {section_type}\"\n",
        "        return {\"output_data\": section_content, \"feedback\": feedback, \"hold_point\": False}\n",
        "\n",
        "def check_grammar(text, rules=None):\n",
        "    errors = []\n",
        "    if \"shall provides\" in text:\n",
        "        errors.append({\"original\": \"shall provides\", \"suggested\": \"shall provide\", \"explanation\": \"Grammar: verb agreement.\"})\n",
        "    # ... more sophisticated checks based on rules ...\n",
        "    return errors\n",
        "\n",
        "class GrammarEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        edits = []\n",
        "        lines = section_content.splitlines()\n",
        "        style_rules = self.context.get(\"style_rules\", {})\n",
        "        for i, line in enumerate(lines):\n",
        "            grammar_errors = check_grammar(line, style_rules.get(\"grammar_checks\"))\n",
        "            for error in grammar_errors:\n",
        "                edits.append({\"line\": i + 1, **error})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Grammar suggestions.\", \"suggested_edits\": edits}\n",
        "\n",
        "def check_spelling(text, dictionary=None):\n",
        "    errors = []\n",
        "    if \"teh\" in text:\n",
        "        errors.append({\"original\": \"teh\", \"suggested\": \"the\", \"explanation\": \"Spelling error.\"})\n",
        "    # ... more sophisticated checks using dictionary ...\n",
        "    return errors\n",
        "\n",
        "class SpellingEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        edits = []\n",
        "        lines = section_content.splitlines()\n",
        "        # Load dictionary from paths_manager.get_input_path(\"spelling_dict.txt\") (example)\n",
        "        for i, line in enumerate(lines):\n",
        "            spelling_errors = check_spelling(line, self.context.get(\"spelling_dictionary\"))\n",
        "            for error in spelling_errors:\n",
        "                edits.append({\"line\": i + 1, **error})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Spelling suggestions.\", \"suggested_edits\": edits}\n",
        "\n",
        "class ConsistencyEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        edits = []\n",
        "        style_rules = self.context.get(\"style_rules\", {})\n",
        "        terminology = style_rules.get(\"terminology_consistency\", {})\n",
        "        for original, preferred in terminology.items():\n",
        "            if original in section_content:\n",
        "                # Basic replacement - needs more sophisticated handling\n",
        "                section_content = section_content.replace(original, preferred)\n",
        "                edits.append({\"original\": original, \"suggested\": preferred, \"explanation\": f\"Consistent use of '{preferred}'.\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Consistency checks.\", \"suggested_edits\": edits}\n",
        "\n",
        "class ClarityEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        suggestions = []\n",
        "        if \"very important\" in section_content:\n",
        "            suggestions.append({\"line\": section_content.find(\"very important\"), \"suggestion\": \"Consider using a more precise term than 'very important'.\", \"type\": \"Clarity\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Clarity suggestions.\", \"clarity_suggestions\": suggestions}\n",
        "\n",
        "class RequirementValidator(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        issues = []\n",
        "        if self.context.get(\"structured_requirements\"):\n",
        "            for req_id, details in self.context[\"structured_requirements\"].items():\n",
        "                if req_id in section_content and \"measurability\" not in section_content.lower():\n",
        "                    issues.append({\"requirement\": req_id, \"issue\": \"Measurability not explicitly mentioned.\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Requirement validation.\", \"validation_issues\": issues}\n",
        "\n",
        "class ContextReviewer(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        suggestions = []\n",
        "        if self.context.get(\"current_section_type\") == \"Requirements Section\" and \"background information\" in section_content.lower():\n",
        "            suggestions.append({\"suggestion\": \"Consider moving background information to a dedicated section.\", \"type\": \"Structure\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Context review.\", \"context_suggestions\": suggestions}\n",
        "\n",
        "class FeedbackAggregator(Agent):\n",
        "    def process(self, section_content: str, all_agent_outputs: List[Dict]) -> Dict:\n",
        "        aggregated_feedback = {\"edits\": [], \"issues\": [], \"suggestions\": [], \"violations\": []}\n",
        "        for output in all_agent_outputs:\n",
        "            aggregated_feedback[\"edits\"].extend(output.get(\"suggested_edits\", []))\n",
        "            aggregated_feedback[\"issues\"].extend(output.get(\"validation_issues\", []))\n",
        "            aggregated_feedback[\"suggestions\"].extend(output.get(\"clarity_suggestions\", []) + output.get(\"context_suggestions\", []))\n",
        "            aggregated_feedback[\"violations\"].extend(output.get(\"violations\", []))\n",
        "\n",
        "        formatted_output = \"### Aggregated Feedback for Section:\\n\"\n",
        "        if aggregated_feedback[\"edits\"]:\n",
        "            formatted_output += \"**Proposed Edits:**\\n\" + \"\\n\".join([f\"- Line {e['line']}: `{e['original']}` -> `{e['suggested']}` ({e['explanation']})\" for e in aggregated_feedback[\"edits\"]])\n",
        "        if aggregated_feedback[\"issues\"]:\n",
        "            formatted_output += \"\\n**Validation Issues:**\\n\" + \"\\n\".join([f\"- {i['requirement']}: {i['issue']}\" for i in aggregated_feedback[\"issues\"]])\n",
        "        if aggregated_feedback[\"suggestions\"]:\n",
        "            formatted_output += \"\\n**Suggestions:**\\n\" + \"\\n\".join([f\"- {s['type']}: {s['suggestion']}\" for s in aggregated_feedback[\"suggestions\"]])\n",
        "        if aggregated_feedback[\"violations\"]:\n",
        "            formatted_output += \"\\n**Style Violations:**\\n\" + \"\\n\".join([f\"- {v}\" for v in aggregated_feedback[\"violations\"]])\n",
        "\n",
        "        return {\"output_data\": section_content, \"feedback\": formatted_output, \"hold_point\": True, \"prompt_for_user\": \"Review aggregated feedback. Accept/Reject/Revise?\"}\n",
        "\n",
        "class FinalComposer(Agent):\n",
        "    def process(self, full_edited_document: str) -> Dict:\n",
        "        # Basic Markdown formatting\n",
        "        final_md = \"# Edited Document\\n\\n\" + full_edited_document\n",
        "        report = f\"# Editing Report\\n\\nGenerated on: {datetime.datetime.now()}\"\n",
        "        return {\"output_data\": final_md, \"feedback\": \"Final document and report generated.\", \"final_document_md\": final_md, \"editing_report_md\": report, \"hold_point\": True, \"prompt_for_user\": \"Review final document and report.\"}\n",
        "\n",
        "class DiagnosticsAgent(Agent):\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        report = []\n",
        "        if \"TODO:\" in document_content:\n",
        "            report.append(\"Warning: Contains 'TODO:' markers.\")\n",
        "        # Example: Check for consistent heading styles (very basic)\n",
        "        headings = [line for line in document_content.splitlines() if line.startswith(\"#\")]\n",
        "        if headings and not all(h.startswith(\"## \") for h in headings):\n",
        "            report.append(\"Suggestion: Consider consistent use of second-level headings (##).\")\n",
        "        return {\"output_data\": document_content, \"feedback\": \"Diagnostics report:\", \"report\": \"\\n\".join(report), \"hold_point\": True, \"prompt_for_user\": \"Review diagnostics report.\"}\n",
        "\n",
        "class HelpersAgent(Agent):\n",
        "    def extract_glossary(self, document_content: str) -> List[str]:\n",
        "        glossary = [line.split(\":\")[1].strip() for line in document_content.splitlines() if line.startswith(\"Term:\")]\n",
        "        return glossary\n",
        "\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        helpers_output = {}\n",
        "        user_input = input(\"Available helpers: extract glossary. Which helper? \").lower()\n",
        "        if \"extract glossary\" in user_input:\n",
        "            helpers_output[\"glossary\"] = self.extract_glossary(document_content)\n",
        "            feedback = f\"Extracted glossary: {helpers_output['glossary']}\"\n",
        "            hold_point = True\n",
        "            prompt_for_user = \"Review extracted glossary.\"\n",
        "        else:\n",
        "            feedback = \"No helper function invoked.\"\n",
        "            hold_point = False\n",
        "            prompt_for_user = None\n",
        "        return {\"output_data\": document_content, \"feedback\": feedback, \"helpers_output\": helpers_output, \"hold_point\": hold_point, \"prompt_for_user\": prompt_for_user}\n",
        "\n",
        "class FixerAgent(Agent):\n",
        "    def process(self, section_content: str, suggested_edits: List[Dict]) -> Dict:\n",
        "        fixed_content = section_content\n",
        "        applied_fixes = []\n",
        "        for edit in suggested_edits:\n",
        "            if edit[\"explanation\"].startswith(\"Spelling:\") and input(f\"Auto-fix '{edit['original']}' to '{edit['suggested']}'? (y/n)\").lower() == 'y':\n",
        "                fixed_content = fixed_content.replace(edit[\"original\"], edit[\"suggested\"], 1)\n",
        "                applied_fixes.append(edit)\n",
        "        return {\"output_data\": fixed_content, \"feedback\": f\"Applied automatic fixes: {applied_fixes}\", \"applied_fixes\": applied_fixes}\n",
        "\n",
        "class CheckerAgent(Agent):\n",
        "    def process(self, section_content: str, style_rules: Dict) -> Dict:\n",
        "        violations = []\n",
        "        if style_rules.get(\"heading_capitalization\") == \"title case\":\n",
        "            for line in section_content.splitlines():\n",
        "                if line.startswith(\"#\") and line != line.title():\n",
        "                    violations.append(f\"Warning: Heading '{line.strip()}' is not title-cased.\")\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Style checks performed.\", \"violations\": violations, \"hold_point\": True, \"prompt_for_user\": \"Review style check violations.\"}\n",
        "\n",
        "class AnalyzerAgent(Agent):\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        readability = len(document_content.split()) / (document_content.count('.') + 1 if document_content.count('.') > 0 else 1) # Very basic\n",
        "        return {\"output_data\": document_content, \"feedback\": f\"Readability: {readability:.2f}\", \"readability_score\": readability, \"hold_point\": False}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting document processing pipeline...\")\n",
        "\n",
        "    # Initialize agents\n",
        "    editor_core = EditorCore(\"EditorCore\")\n",
        "    structured_data_loader = StructuredDataLoader(\"StructuredDataLoader\")\n",
        "    style_guide_processor = StyleGuideProcessor(\"StyleGuideProcessor\")\n",
        "    structural_analyzer = StructuralAnalyzer(\"StructuralAnalyzer\")\n",
        "    grammar_editor = GrammarEditor(\"GrammarEditor\")\n",
        "    spelling_editor = SpellingEditor(\"SpellingEditor\")\n",
        "    consistency_editor = ConsistencyEditor(\"ConsistencyEditor\")\n",
        "    clarity_editor = ClarityEditor(\"ClarityEditor\")\n",
        "    requirement_validator = RequirementValidator(\"RequirementValidator\")\n",
        "    context_reviewer = ContextReviewer(\"ContextReviewer\")\n",
        "    feedback_aggregator = FeedbackAggregator(\"FeedbackAggregator\")\n",
        "    final_composer = FinalComposer(\"FinalComposer\")\n",
        "    diagnostics_agent = DiagnosticsAgent(\"DiagnosticsAgent\")\n",
        "    helpers_agent = HelpersAgent(\"HelpersAgent\")\n",
        "    fixer_agent = FixerAgent(\"FixerAgent\")\n",
        "    checker_agent = CheckerAgent(\"CheckerAgent\")\n",
        "    analyzer_agent = AnalyzerAgent(\"AnalyzerAgent\")\n",
        "\n",
        "\n",
        "    agents = [\n",
        "        editor_core,\n",
        "        structured_data_loader,\n",
        "        style_guide_processor,\n",
        "        structural_analyzer,\n",
        "        grammar_editor,\n",
        "        spelling_editor,\n",
        "        consistency_editor,\n",
        "        clarity_editor,\n",
        "        requirement_validator,\n",
        "        context_reviewer,\n",
        "        feedback_aggregator,\n",
        "        final_composer,\n",
        "        diagnostics_agent,\n",
        "        helpers_agent,\n",
        "        fixer_agent,\n",
        "        checker_agent,\n",
        "        analyzer_agent,\n",
        "\n",
        "    ]\n",
        "\n",
        "    shared_context = {}\n",
        "    current_document_content = \"\"\n",
        "\n",
        "    # Load input document\n",
        "    input_filename = \"input_document.md\" # Example filename\n",
        "    input_path = paths_manager.get_input_path(input_filename)\n",
        "\n",
        "    if not os.path.exists(input_path):\n",
        "        print(f\"Error: Input file not found at {input_path}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        with open(input_path, 'r', encoding='utf-8') as f:\n",
        "            current_document_content = f.read()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading input file: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"Loaded document: {input_filename}\")\n",
        "\n",
        "    # --- Pipeline Execution ---\n",
        "    processed_content = current_document_content\n",
        "    sectioned_content = processed_content.split(\"\\n## \") # Very basic sectioning\n",
        "    if not sectioned_content[0].startswith(\"#\"):\n",
        "        sectioned_content[0] = \"## \" + sectioned_content[0] # Ensure first part is treated as a section\n",
        "\n",
        "    full_edited_document_sections = []\n",
        "\n",
        "    for i, section in enumerate(sectioned_content):\n",
        "        if i > 0:\n",
        "            section = \"## \" + section # Add back the section marker lost in split\n",
        "\n",
        "        print(f\"\\n--- Processing Section {i+1} ---\")\n",
        "        current_section_content = section\n",
        "        section_agent_outputs = []\n",
        "\n",
        "        # Non-section-specific agents process the whole document or context first\n",
        "        if i == 0: # Process document-level agents once\n",
        "            print(\"Running document-level agents...\")\n",
        "            for agent in [editor_core, structured_data_loader, style_guide_processor, diagnostics_agent, analyzer_agent]:\n",
        "                agent.set_context(shared_context)\n",
        "                output = agent.process(current_document_content)\n",
        "                shared_context.update(agent.context) # Update shared context with agent's context\n",
        "                section_agent_outputs.append(output)\n",
        "                print(f\"- {agent.name}: {output.get('feedback', 'Processed')}\")\n",
        "                if output.get(\"hold_point\"):\n",
        "                    print(f\"HOLD POINT: {output.get('prompt_for_user', 'User interaction required.')}\")\n",
        "                    input(\"Press Enter to continue...\") # Simple blocking hold\n",
        "\n",
        "            # Load structured data and style guide if they exist\n",
        "            structured_data_path = paths_manager.get_input_path(\"structured_requirements.json\")\n",
        "            if os.path.exists(structured_data_path):\n",
        "                 try:\n",
        "                     with open(structured_data_path, 'r') as f:\n",
        "                         shared_context[\"structured_requirements\"] = json.load(f)\n",
        "                     print(\"Loaded structured requirements.\")\n",
        "                 except json.JSONDecodeError:\n",
        "                     print(\"Warning: Could not load structured requirements JSON.\")\n",
        "\n",
        "            style_guide_path = paths_manager.get_input_path(\"style_guide.json\")\n",
        "            if os.path.exists(style_guide_path):\n",
        "                try:\n",
        "                    with open(style_guide_path, 'r') as f:\n",
        "                        shared_context[\"style_rules\"] = json.load(f)\n",
        "                    print(\"Loaded style guide rules.\")\n",
        "                except json.JSONDecodeError:\n",
        "                    print(\"Warning: Could not load style guide JSON.\")\n",
        "\n",
        "\n",
        "        # Process section-specific agents\n",
        "        section_agents = [\n",
        "            structural_analyzer,\n",
        "            grammar_editor,\n",
        "            spelling_editor,\n",
        "            consistency_editor,\n",
        "            clarity_editor,\n",
        "            requirement_validator,\n",
        "            context_reviewer,\n",
        "            checker_agent, # Style checker per section\n",
        "        ]\n",
        "\n",
        "        for agent in section_agents:\n",
        "            agent.set_context(shared_context)\n",
        "            output = agent.process(current_section_content)\n",
        "            shared_context.update(agent.context)\n",
        "            section_agent_outputs.append(output)\n",
        "            print(f\"- {agent.name}: {output.get('feedback', 'Processed')}\")\n",
        "            if output.get(\"hold_point\"):\n",
        "                print(f\"HOLD POINT: {output.get('prompt_for_user', 'User interaction required.')}\")\n",
        "                input(\"Press Enter to continue...\")\n",
        "\n",
        "\n",
        "        # Aggregation and Fixer (could be iterative)\n",
        "        print(\"\\nRunning Feedback Aggregator...\")\n",
        "        feedback_aggregator.set_context(shared_context)\n",
        "        aggregated_feedback_output = feedback_aggregator.process(current_section_content, section_agent_outputs)\n",
        "        section_agent_outputs.append(aggregated_feedback_output)\n",
        "        print(f\"- {feedback_aggregator.name}: {aggregated_feedback_output.get('feedback', 'Aggregated')}\")\n",
        "        if aggregated_feedback_output.get(\"hold_point\"):\n",
        "            print(f\"HOLD POINT: {aggregated_feedback_output.get('prompt_for_user', 'User interaction required.')}\")\n",
        "            user_action = input(\"Enter 'accept' to apply all edits, 'fix' to run Fixer: \").lower()\n",
        "\n",
        "            if user_action == 'fix':\n",
        "                print(\"\\nRunning Fixer Agent...\")\n",
        "                fixer_agent.set_context(shared_context)\n",
        "                fixer_output = fixer_agent.process(current_section_content, aggregated_feedback_output.get(\"edits\", []))\n",
        "                current_section_content = fixer_output[\"output_data\"]\n",
        "                print(f\"- {fixer_agent.name}: {fixer_output.get('feedback', 'Fixed')}\")\n",
        "            elif user_action == 'accept':\n",
        "                 # Simple apply all suggested edits (needs more sophisticated implementation)\n",
        "                 for edit in aggregated_feedback_output.get(\"edits\", []):\n",
        "                     # This is a simplistic replace; a real implementation needs line-by-line precision\n",
        "                     current_section_content = current_section_content.replace(edit['original'], edit['suggested'], 1)\n",
        "                 print(\"Applied all suggested edits.\")\n",
        "\n",
        "\n",
        "        full_edited_document_sections.append(current_section_content)\n",
        "\n",
        "    # Reconstruct the full document\n",
        "    full_edited_document = \"\\n\".join(full_edited_document_sections)\n",
        "\n",
        "    # Final Composition\n",
        "    print(\"\\nRunning Final Composer...\")\n",
        "    final_composer.set_context(shared_context)\n",
        "    final_output = final_composer.process(full_edited_document)\n",
        "    print(f\"- {final_composer.name}: {final_output.get('feedback', 'Composed')}\")\n",
        "    if final_output.get(\"hold_point\"):\n",
        "        print(f\"HOLD POINT: {final_output.get('prompt_for_user', 'User interaction required.')}\")\n",
        "        input(\"Press Enter to generate output files...\")\n",
        "\n",
        "    # Save output files\n",
        "    output_document_path = paths_manager.get_output_path(\"edited_document.md\")\n",
        "    editing_report_path = paths_manager.get_output_path(\"editing_report.md\")\n",
        "\n",
        "    try:\n",
        "        with open(output_document_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(final_output.get(\"final_document_md\", full_edited_document))\n",
        "        print(f\"Edited document saved to {output_document_path}\")\n",
        "\n",
        "        with open(editing_report_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(final_output.get(\"editing_report_md\", \"No report generated.\"))\n",
        "        print(f\"Editing report saved to {editing_report_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing output files: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Backup input and output folders (example using shutil)\n",
        "    backup_input_path = paths_manager.get_backup_input_path()\n",
        "    backup_output_path = paths_manager.get_backup_output_path()\n",
        "\n",
        "    try:\n",
        "        shutil.copytree(paths_manager.input_folder, backup_input_path)\n",
        "        print(f\"Backed up input folder to {backup_input_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not backup input folder - {e}\")\n",
        "\n",
        "    try:\n",
        "        shutil.copytree(paths_manager.output_folder, backup_output_path)\n",
        "        print(f\"Backed up output folder to {backup_output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not backup output folder - {e}\")\n",
        "\n",
        "    print(\"\\nProcessing complete.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "rujARnKtYRSS",
        "outputId": "f65b173e-ecad-49ed-a753-07a8d053e544"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting document processing pipeline...\n",
            "Loaded document: input_document.md\n",
            "\n",
            "--- Processing Section 1 ---\n",
            "Running document-level agents...\n",
            "- EditorCore: Initial assessment: Document length approx. 41 words. Identified as 'Technical Requirements Document (SoR)'.\n",
            "Confirm?\n",
            "HOLD POINT: Confirm document type and goals.\n",
            "Press Enter to continue...\n",
            "Press Enter to continue...\n",
            "- StructuredDataLoader: Error loading structured data: Expecting value: line 1 column 1 (char 0)\n",
            "HOLD POINT: Review parsed structured data.\n",
            "- StyleGuideProcessor: Style rules loaded/inferred: {}\n",
            "HOLD POINT: Review and confirm style rules.\n",
            "Press Enter to continue...\n",
            "- DiagnosticsAgent: Diagnostics report:\n",
            "HOLD POINT: Review diagnostics report.\n",
            "Press Enter to continue...\n",
            "- AnalyzerAgent: Readability: 10.25\n",
            "- StructuralAnalyzer: Analyzed section structure: Type - unknown\n",
            "- GrammarEditor: Grammar suggestions.\n",
            "- SpellingEditor: Spelling suggestions.\n",
            "- ConsistencyEditor: Consistency checks.\n",
            "- ClarityEditor: Clarity suggestions.\n",
            "- RequirementValidator: Requirement validation.\n",
            "- ContextReviewer: Context review.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "CheckerAgent.process() missing 1 required positional argument: 'style_rules'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-417bf9009cff>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msection_agents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_section_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m             \u001b[0mshared_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0msection_agent_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: CheckerAgent.process() missing 1 required positional argument: 'style_rules'"
          ]
        }
      ]
    },
    {
      "source": [
        "# Create the input directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(\"input\", exist_ok=True)\n",
        "\n",
        "# Create a dummy input file\n",
        "dummy_content = \"\"\"# My Document\n",
        "\n",
        "## Introduction\n",
        "This is a sample document.\n",
        "\n",
        "## Requirements\n",
        "Requirement 1: The system shall provide a user interface.\n",
        "Requirement 2: The system shall be measurable.\n",
        "\n",
        "## Glossary\n",
        "Term: API - Application Programming Interface\n",
        "Term: UI - User Interface\n",
        "\"\"\"\n",
        "\n",
        "input_filename = \"input_document.md\"\n",
        "input_path = os.path.join(\"input\", input_filename)\n",
        "\n",
        "with open(input_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(dummy_content)\n",
        "\n",
        "print(f\"Created dummy input file at {input_path}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "zYBeLfrhZE_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was is working - B_FILE"
      ],
      "metadata": {
        "id": "GW2MDDCpYFGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import datetime\n",
        "import subprocess\n",
        "import json\n",
        "import re  # For more advanced text processing\n",
        "from typing import Dict, List # Import Dict and List for type hinting\n",
        "# from docx import Document # For potential .docx handling (conceptual)\n",
        "\n",
        "# --- Configuration (remains largely the same) ---\n",
        "CONFIG_FILE = \"config.json\"\n",
        "DEFAULT_INPUT_FOLDER = \"input\"\n",
        "DEFAULT_OUTPUT_FOLDER = \"output\"\n",
        "DEFAULT_BACKUP_INPUT_ONEDRIVE = os.path.expanduser(\"~/OneDrive/Backup/Input\")\n",
        "DEFAULT_BACKUP_OUTPUT_ONEDRIVE = os.path.expanduser(\"~/OneDrive/Backup/Output\")\n",
        "\n",
        "class PathsManager:\n",
        "    # Added pass statement to satisfy indentation requirements\n",
        "    pass\n",
        "    # ... (remains largely the same, but could be extended for temp files, etc.) ...\n",
        "\n",
        "paths_manager = PathsManager()\n",
        "\n",
        "class Agent:\n",
        "    # ... (remains the same) ...\n",
        "    def __init__(self, context: Dict):\n",
        "        self.context = context\n",
        "\n",
        "    def process(self, *args, **kwargs) -> Dict:\n",
        "        raise NotImplementedError(\"Subclasses must implement abstract method\")\n",
        "\n",
        "\n",
        "class EditorCore(Agent):\n",
        "    # EditorCore class also needs an indented block - Added pass\n",
        "    pass # Add pass as a placeholder\n",
        "\n",
        "class StructuredDataLoader(Agent):\n",
        "    def process(self, structured_data_content: str) -> Dict:\n",
        "        parsed_data = {}\n",
        "        try:\n",
        "            if structured_data_content.lower().endswith(('.json')):\n",
        "                parsed_data = json.loads(structured_data_content)\n",
        "            # elif structured_data_content.lower().endswith(('.yaml', '.yml')):\n",
        "            #     import yaml # Conditional import (conceptual)\n",
        "            #     parsed_data = yaml.safe_load(structured_data_content)\n",
        "            # elif structured_data_content.lower().endswith(('.mp')):\n",
        "            #     # Placeholder for Master.mp parsing (conceptual)\n",
        "            #     parsed_data = {\"master_data\": \"parsed\"}\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported structured data format.\")\n",
        "            self.context.update({\"structured_requirements\": parsed_data, \"structured_data_valid\": True})\n",
        "            feedback = f\"Structured data loaded successfully from format: {os.path.splitext(structured_data_content)[1][1:]}\"\n",
        "        except (json.JSONDecodeError, ValueError) as e:\n",
        "            self.context.update({\"structured_data_valid\": False})\n",
        "            feedback = f\"Error loading structured data: {e}\"\n",
        "        return {\"output_data\": structured_data_content, \"feedback\": feedback, \"hold_point\": True, \"prompt_for_user\": \"Review parsed structured data.\"}\n",
        "\n",
        "class StyleGuideProcessor(Agent):\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        style_rules = {}\n",
        "        # Assuming get_input_path is defined in PathsManager or elsewhere in the actual code\n",
        "        # If not, this will cause a NameError later.\n",
        "        # style_guide_path = paths_manager.get_input_path(\"style_guide.json\")\n",
        "        # For now, using a placeholder path\n",
        "        style_guide_path = \"input/style_guide.json\" # Placeholder\n",
        "\n",
        "        if os.path.exists(style_guide_path):\n",
        "            try:\n",
        "                with open(style_guide_path, 'r') as f:\n",
        "                    style_rules = json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                print(\"Warning: Could not load style guide.\")\n",
        "\n",
        "        # More mature inference (conceptual): analyze document for consistent casing, terminology\n",
        "        inferred_terminology = {}\n",
        "        words = re.findall(r'\\b\\w+\\b', document_content)\n",
        "        for word in set(words):\n",
        "            if words.count(word.upper()) > words.count(word.lower()):\n",
        "                inferred_terminology[word.lower()] = word.upper() # Suggest uppercase if more frequent\n",
        "\n",
        "        final_rules = style_rules.copy()\n",
        "        final_rules.setdefault(\"terminology_consistency\", {}).update(inferred_terminology)\n",
        "        self.context.update({\"style_rules\": final_rules})\n",
        "        feedback = f\"Style rules loaded/inferred: {final_rules}\"\n",
        "        return {\"output_data\": document_content, \"feedback\": feedback, \"hold_point\": True, \"prompt_for_user\": \"Review and confirm style rules.\"}\n",
        "\n",
        "class StructuralAnalyzer(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        section_type = \"unknown\"\n",
        "        if re.search(r\"^(##?\\s+Requirements|##?\\s+Functional Specifications)\", section_content, re.IGNORECASE):\n",
        "            section_type = \"Requirements Section\"\n",
        "        elif re.search(r\"^(##?\\s+Glossary|##?\\s+Definitions)\", section_content, re.IGNORECASE):\n",
        "            section_type = \"Glossary Section\"\n",
        "        elif re.search(r\"^(##?\\s+Introduction|##?\\s+Background)\", section_content, re.IGNORECASE):\n",
        "            section_type = \"Introduction Section\"\n",
        "        self.context.update({\"current_section_type\": section_type})\n",
        "        feedback = f\"Analyzed section structure: Type - {section_type}\"\n",
        "        return {\"output_data\": section_content, \"feedback\": feedback, \"hold_point\": False}\n",
        "\n",
        "def check_grammar(text, rules=None):\n",
        "    errors = []\n",
        "    # More mature check (conceptual): use a library like `language_tool_python`\n",
        "    # import language_tool_python # Conditional import (conceptual)\n",
        "    # tool = language_tool_python.LanguageTool('en-US')\n",
        "    # matches = tool.check(text)\n",
        "    # for match in matches:\n",
        "    #     errors.append({\"original\": text[match.offset:match.offset + match.errorLength],\n",
        "    #                    \"suggested\": match.replacements,\n",
        "    #                    \"explanation\": match.message})\n",
        "    if \"shall provides\" in text:\n",
        "        errors.append({\"original\": \"shall provides\", \"suggested\": [\"shall provide\"], \"explanation\": \"Grammar: verb agreement.\"})\n",
        "    return errors\n",
        "\n",
        "class GrammarEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        edits = []\n",
        "        lines = section_content.splitlines()\n",
        "        style_rules = self.context.get(\"style_rules\", {})\n",
        "        for i, line in enumerate(lines):\n",
        "            grammar_errors = check_grammar(line, style_rules.get(\"grammar_checks\"))\n",
        "            for error in grammar_errors:\n",
        "                edits.append({\"line\": i + 1, \"original\": error[\"original\"], \"suggested\": error[\"suggested\"][0] if error[\"suggested\"] else \"\", \"explanation\": error[\"explanation\"]})\n",
        "        return {\"output_data\": section_content, \"feedback\": f\"{len(edits)} grammar suggestions.\", \"suggested_edits\": edits}\n",
        "\n",
        "def check_spelling(text, dictionary_path=None):\n",
        "    errors = []\n",
        "    # More mature check (conceptual): use a library like `pyspellchecker`\n",
        "    # from spellchecker import SpellChecker # Conditional import (conceptual)\n",
        "    # spell = SpellChecker()\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    # misspelled = spell.unknown(words)\n",
        "    if \"teh\" in text.lower():\n",
        "        errors.append({\"original\": \"teh\", \"suggested\": \"the\", \"explanation\": \"Spelling error.\"})\n",
        "    return errors\n",
        "\n",
        "class SpellingEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        edits = []\n",
        "        lines = section_content.splitlines()\n",
        "        # Assuming get_input_path is defined in PathsManager or elsewhere in the actual code\n",
        "        # dictionary_path = paths_manager.get_input_path(\"spelling_dict.txt\") # Conceptual dictionary\n",
        "        # For now, using a placeholder path\n",
        "        dictionary_path = \"input/spelling_dict.txt\" # Placeholder\n",
        "        dictionary = set()\n",
        "        if os.path.exists(dictionary_path):\n",
        "            with open(dictionary_path, 'r') as f:\n",
        "                dictionary.update(f.read().splitlines())\n",
        "        for i, line in enumerate(lines):\n",
        "            spelling_errors = check_spelling(line, dictionary)\n",
        "            for error in spelling_errors:\n",
        "                edits.append({\"line\": i + 1, **error})\n",
        "        return {\"output_data\": section_content, \"feedback\": f\"{len(edits)} spelling suggestions.\", \"suggested_edits\": edits}\n",
        "\n",
        "class ConsistencyEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        edits = []\n",
        "        style_rules = self.context.get(\"style_rules\", {})\n",
        "        terminology = style_rules.get(\"terminology_consistency\", {})\n",
        "        for original, preferred in terminology.items():\n",
        "            # More mature replacement: case-insensitive with whole word matching\n",
        "            pattern = r'\\b' + re.escape(original) + r'\\b'\n",
        "            new_content, num_replaced = re.subn(pattern, preferred, section_content, flags=re.IGNORECASE)\n",
        "            if num_replaced > 0:\n",
        "                edits.append({\"original\": original, \"suggested\": preferred, \"explanation\": f\"Consistent use of '{preferred}' ({num_replaced} instances).\"})\n",
        "                section_content = new_content\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Consistency checks.\", \"suggested_edits\": edits}\n",
        "\n",
        "class ClarityEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        suggestions = []\n",
        "        if re.search(r\"\\b(very|really|quite|somewhat)\\s+(important|significant)\\b\", section_content, re.IGNORECASE):\n",
        "            match = re.search(r\"\\b(very|really|quite|somewhat)\\s+(important|significant)\\b\", section_content, re.IGNORECASE)\n",
        "            # Corrected line finding to use the start position of the match\n",
        "            line_number = section_content[:match.start()].count('\\n') + 1\n",
        "            suggestions.append({\"line\": line_number, \"suggestion\": f\"Consider a more precise adjective than '{match.group()}'.\", \"type\": \"Clarity\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": f\"{len(suggestions)} clarity suggestions.\", \"clarity_suggestions\": suggestions}\n",
        "\n",
        "class RequirementValidator(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        issues = []\n",
        "        structured_requirements = self.context.get(\"structured_requirements\", {})\n",
        "        for req_id, details in structured_requirements.items():\n",
        "            if re.search(r\"\\b\" + re.escape(req_id) + r\"\\b\", section_content) and not re.search(r\"(shall|must|is required to)\\b.*\\b(be measured|verify|test|demonstrate)\\b\", section_content, re.IGNORECASE):\n",
        "                issues.append({\"requirement\": req_id, \"issue\": \"Measurability criteria might be missing.\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": f\"{len(issues)} requirement validation issues.\", \"validation_issues\": issues}\n",
        "\n",
        "class ContextReviewer(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        suggestions = []\n",
        "        current_section_type = self.context.get(\"current_section_type\")\n",
        "        if current_section_type == \"Requirements Section\" and re.search(r\"(e\\.g\\.|i\\.e\\.|for example)\\s+[^.]+\\s+background\", section_content, re.IGNORECASE):\n",
        "            suggestions.append({\"suggestion\": \"Consider separating examples from core requirements.\", \"type\": \"Structure\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": f\"{len(suggestions)} context review suggestions.\", \"context_suggestions\": suggestions}\n",
        "\n",
        "class FeedbackAggregator(Agent):\n",
        "    def process(self, section_content: str, all_agent_outputs: List[Dict]) -> Dict:\n",
        "        aggregated_feedback = {\"edits\": [], \"issues\": [], \"suggestions\": [], \"violations\": []}\n",
        "        for output in all_agent_outputs:\n",
        "            aggregated_feedback[\"edits\"].extend(output.get(\"suggested_edits\", []))\n",
        "            aggregated_feedback[\"issues\"].extend(output.get(\"validation_issues\", []))\n",
        "            aggregated_feedback[\"suggestions\"].extend(output.get(\"clarity_suggestions\", []) + output.get(\"context_suggestions\", []))\n",
        "            aggregated_feedback[\"violations\"].extend(output.get(\"violations\", []))\n",
        "\n",
        "        formatted_output = \"### Aggregated Feedback for Section:\\n\"\n",
        "        if aggregated_feedback[\"edits\"]:\n",
        "            formatted_output += \"**Proposed Edits:**\\n\" + \"\\n\".join([f\"- Line {e['line']}: `{e['original']}` -> `{e['suggested']}` ({e['explanation']})\" for e in aggregated_feedback[\"edits\"]])\n",
        "        if aggregated_feedback[\"issues\"]:\n",
        "            formatted_output += \"\\n**Validation Issues:**\\n\" + \"\\n\".join([f\"- {i['requirement']}: {i['issue']}\" for i in aggregated_feedback[\"issues\"]])\n",
        "        if aggregated_feedback[\"suggestions\"]:\n",
        "            formatted_output += \"\\n**Suggestions:**\\n\" + \"\\n\".join([f\"- {s['type']}: {s['suggestion']}\" for s in aggregated_feedback[\"suggestions\"]])\n",
        "        if aggregated_feedback[\"violations\"]:\n",
        "            formatted_output += \"\\n**Style Violations:**\\n\" + \"\\n\".join([f\"- {v}\" for v in aggregated_feedback[\"violations\"]])\n",
        "\n",
        "        return {\"output_data\": section_content, \"feedback\": formatted_output, \"hold_point\": True, \"prompt_for_user\": \"Review aggregated feedback. Accept/Reject/Revise?\"}\n",
        "\n",
        "class FinalComposer(Agent):\n",
        "    def process(self, full_edited_document: str) -> Dict:\n",
        "        # More mature Markdown formatting (conceptual)\n",
        "        final_md = \"# Edited Document\\n\\n\"\n",
        "        sections = full_edited_document.split(\"\\n## \")\n",
        "        for section in sections:\n",
        "            if section.strip():\n",
        "                level = 2 if section.startswith(\"#\") else 1\n",
        "                title = section.split(\"\\n\")[0].lstrip(\"#\").strip()\n",
        "                content = \"\\n\".join(section.split(\"\\n\")[1:])\n",
        "                final_md += f\"{'#' * level} {title}\\n\\n{content}\\n\\n\"\n",
        "\n",
        "        report = f\"# Editing Report\\n\\nGenerated on: {datetime.datetime.now()}\\n\\n\" \\\n",
        "                 f\"Document Type: {self.context.get('document_type', 'N/A')}\\n\" \\\n",
        "                 f\"Total Edits: ... (conceptual)\\n\" \\\n",
        "                 f\"Total Suggestions: ... (conceptual)\\n\" \\\n",
        "                 f\"Validation Issues Found: ... (conceptual)\\n\"\n",
        "        return {\"output_data\": final_md, \"feedback\": \"Final document and report generated.\", \"final_document_md\": final_md, \"editing_report_md\": report, \"hold_point\": True, \"prompt_for_user\": \"Review final document and report.\"}\n",
        "\n",
        "class DiagnosticsAgent(Agent):\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        report = []\n",
        "        if re.search(r\"\\[\\s*citation needed\\s*\\]\", document_content, re.IGNORECASE):\n",
        "            report.append(\"Warning: Contains '[citation needed]' placeholders.\")\n",
        "        # More mature check: inconsistent heading levels\n",
        "        heading_levels = [line.count('#') for line in document_content.splitlines() if line.startswith(\"#\")]\n",
        "        if heading_levels and any(abs(heading_levels[i] - heading_levels[i+1]) > 1 for i in range(len(heading_levels) - 1)):\n",
        "            report.append(\"Suggestion: Check for inconsistent heading level jumps.\")\n",
        "        return {\"output_data\": document_content, \"feedback\": \"Diagnostics report:\", \"report\": \"\\n\".join(report), \"hold_point\": True, \"prompt_for_user\": \"Review diagnostics report.\"}\n",
        "\n",
        "class HelpersAgent(Agent):\n",
        "    def extract_glossary(self, document_content: str) -> List[str]:\n",
        "        # Corrected the syntax error in the list comprehension\n",
        "        glossary = [re.search(r\"^Term:\\s*(.*)\", line, re.IGNORECASE).group(1).strip()\n",
        "                    for line in document_content.splitlines() if re.match(r\"^Term:\\s*\", line, re.IGNORECASE)]\n",
        "        return sorted(list(set(glossary)))\n",
        "\n",
        "    def expand_acronyms(self, document_content: str, acronym_list_path=\"data/acronyms.json\"):\n",
        "        acronyms = {}\n",
        "        if os.path.exists(acronym_list_path):\n",
        "            try:\n",
        "                with open(acronym_list_path, 'r') as f:\n",
        "                    acronyms = json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                print(\"Warning: Could not load acronym list.\")\n",
        "        expanded_content = document_content\n",
        "        for acronym, expansion in acronyms.items():\n",
        "            pattern = r'\\b' + re.escape(acronym) + r'\\b'\n",
        "            expanded_content = re.sub(pattern, f\"{acronym} ({expansion})\", expanded_content)\n",
        "        return expanded_content\n",
        "\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        helpers_output = {}\n",
        "        # Added a default value to input() to avoid error if user just presses Enter\n",
        "        helper_choice = input(\"Available helpers: extract glossary, expand acronyms. Which helper? (or 'none') \").lower() or 'none'\n",
        "        if helper_choice == \"extract glossary\":\n",
        "            helpers_output[\"glossary\"] = self.extract_glossary(document_content)\n",
        "            feedback = f\"Extracted glossary: {helpers_output.get('glossary', 'N/A')}\" # Use .get for safety\n",
        "            hold_point = True\n",
        "            prompt_for_user = \"Review extracted glossary.\"\n",
        "            output_data = document_content # Keep original content unless modified\n",
        "        elif helper_choice == \"expand acronyms\":\n",
        "            expanded_content = self.expand_acronyms(document_content)\n",
        "            helpers_output[\"expanded_content\"] = expanded_content\n",
        "            feedback = \"Acronyms expanded (if list provided).\"\n",
        "            hold_point = True\n",
        "            prompt_for_user = \"Review content with expanded acronyms.\"\n",
        "            output_data = expanded_content # Use expanded content\n",
        "        else:\n",
        "            feedback = \"No helper function invoked.\"\n",
        "            hold_point = False\n",
        "            prompt_for_user = None\n",
        "            output_data = document_content # Keep original content\n",
        "\n",
        "        # Ensure a dictionary is always returned\n",
        "        return {\"output_data\": output_data,\n",
        "                \"feedback\": feedback,\n",
        "                \"hold_point\": hold_point,\n",
        "                \"prompt_for_user\": prompt_for_user,\n",
        "                **helpers_output # Include any specific helper output\n",
        "                }"
      ],
      "metadata": {
        "id": "hNL2TE1rVODW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HrWn7vhhYw2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}