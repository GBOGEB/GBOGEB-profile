{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GBOGEB/GBOGEB-profile/blob/main/AGENT_0606_GG\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, I understand. You want a more detailed expansion and revision of the previous DMAIC breakdown, including:\n",
        "\n",
        "* **Specific Code Examples:** Concrete implementations for key parts of `main.py`, helpers, and sub-functions.\n",
        "* **Enhanced Workflow and ASCII:** A more detailed ASCII representation showing the flow between the new blocks/functions/features.\n",
        "* **Updated Requirements/Rules:** Refinements to the `requires` and `rules` sections of the JSON.\n",
        "* **Fuller Repo Structure:** Elaboration on the purpose and potential content of the different directories and files.\n",
        "\n",
        "Let's proceed with this expanded and revised approach, building upon the previous information.\n",
        "\n",
        "**D - Define (Revisited):**\n",
        "\n",
        "The core goal remains enhancing the \"Schrijfeditor\" with robustness, user-friendliness, and advanced editing tools. We're now focusing on providing concrete implementations and clearer visual representations.\n",
        "\n",
        "**M - Measure (Revisited):**\n",
        "\n",
        "The metrics for success still revolve around improved user experience, reduced errors, and increased efficiency. The key is to see how the implemented code and workflow directly contribute to these improvements.\n",
        "\n",
        "**A - Analyze (Revisited):**\n",
        "\n",
        "The analysis still points to the need for better path management, automated checks, helpful utilities, and robust error handling. The opportunity lies in translating these needs into functional code and a clear workflow.\n",
        "\n",
        "**I - Improve (Expanded):**\n",
        "\n",
        "Let's delve into the specific code and workflow enhancements.\n",
        "\n",
        "**1. Updated `cli.py` Code with Examples:**"
      ],
      "metadata": {
        "id": "TQvy5RuBTmGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import datetime\n",
        "import subprocess\n",
        "import json\n",
        "\n",
        "# --- Configuration ---\n",
        "CONFIG_FILE = \"config.json\"\n",
        "DEFAULT_INPUT_FOLDER = \"input\"\n",
        "DEFAULT_OUTPUT_FOLDER = \"output\"\n",
        "DEFAULT_BACKUP_INPUT_ONEDRIVE = os.path.expanduser(\"~/OneDrive/Backup/Input\")\n",
        "DEFAULT_BACKUP_OUTPUT_ONEDRIVE = os.path.expanduser(\"~/OneDrive/Backup/Output\")\n",
        "\n",
        "class PathsManager:\n",
        "    def __init__(self, config_file=CONFIG_FILE):\n",
        "        self.config = self._load_config(config_file)\n",
        "        self.input_folder = self.config.get(\"input_folder\", DEFAULT_INPUT_FOLDER)\n",
        "        self.output_folder = self.config.get(\"output_folder\", DEFAULT_OUTPUT_FOLDER)\n",
        "        self.backup_input_onedrive = self.config.get(\"backup_input_onedrive\", DEFAULT_BACKUP_INPUT_ONEDRIVE)\n",
        "        self.backup_output_onedrive = self.config.get(\"backup_output_onedrive\", DEFAULT_BACKUP_OUTPUT_ONEDRIVE)\n",
        "        self._ensure_paths_exist()\n",
        "\n",
        "    def _load_config(self, config_file):\n",
        "        if os.path.exists(config_file):\n",
        "            try:\n",
        "                with open(config_file, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Warning: Could not decode config file '{config_file}'. Using defaults.\")\n",
        "                return {}\n",
        "        return {}\n",
        "\n",
        "    def _ensure_paths_exist(self):\n",
        "        os.makedirs(self.input_folder, exist_ok=True)\n",
        "        os.makedirs(self.output_folder, exist_ok=True)\n",
        "        # Note: We don't automatically create OneDrive backup paths\n",
        "\n",
        "    def get_input_path(self, filename=\"\"):\n",
        "        return os.path.join(self.input_folder, filename)\n",
        "\n",
        "    def get_output_path(self, filename=\"\"):\n",
        "        return os.path.join(self.output_folder, filename)\n",
        "\n",
        "    def get_backup_input_path(self, folder_name=\"input\"):\n",
        "        return os.path.join(self.backup_input_onedrive, folder_name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
        "\n",
        "    def get_backup_output_path(self, folder_name=\"output\"):\n",
        "        return os.path.join(self.backup_output_onedrive, folder_name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
        "\n",
        "paths_manager = PathsManager()\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name: str):\n",
        "        self.name = name\n",
        "        self.context: Dict = {}\n",
        "\n",
        "    def set_context(self, context: Dict):\n",
        "        self.context = context\n",
        "\n",
        "    def process(self, input_data: Any) -> Dict:\n",
        "        raise NotImplementedError\n",
        "\n",
        "class EditorCore(Agent):\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        doc_length = len(document_content.split())\n",
        "        doc_type = \"Technical Requirements Document (SoR)\"\n",
        "        feedback = f\"Initial assessment: Document length approx. {doc_length} words. Identified as '{doc_type}'.\\nConfirm?\"\n",
        "        self.context.update({\"document_type\": doc_type, \"doc_length\": doc_length})\n",
        "        return {\"output_data\": document_content, \"feedback\": feedback, \"hold_point\": True, \"prompt_for_user\": \"Confirm document type and goals.\"}\n",
        "\n",
        "class StructuredDataLoader(Agent):\n",
        "    def process(self, structured_data_content: str) -> Dict:\n",
        "        try:\n",
        "            parsed_data = json.loads(structured_data_content) # Example: Assuming JSON\n",
        "            self.context.update({\"structured_requirements\": parsed_data, \"structured_data_valid\": True})\n",
        "            feedback = \"Structured data loaded successfully.\"\n",
        "        except json.JSONDecodeError as e:\n",
        "            self.context.update({\"structured_data_valid\": False})\n",
        "            feedback = f\"Error loading structured data: {e}\"\n",
        "        return {\"output_data\": structured_data_content, \"feedback\": feedback, \"hold_point\": True, \"prompt_for_user\": \"Review parsed structured data.\"}\n",
        "\n",
        "class StyleGuideProcessor(Agent):\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        # Load from data/style_guide.json (example)\n",
        "        style_rules = {}\n",
        "        style_guide_path = paths_manager.get_input_path(\"style_guide.json\") # Assuming in input for simplicity\n",
        "        if os.path.exists(style_guide_path):\n",
        "            try:\n",
        "                with open(style_guide_path, 'r') as f:\n",
        "                    style_rules = json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                print(\"Warning: Could not load style guide.\")\n",
        "        inferred_rules = self.context.get(\"style_rules\", style_rules) # Prioritize loaded\n",
        "        self.context.update({\"style_rules\": inferred_rules})\n",
        "        feedback = f\"Style rules loaded/inferred: {inferred_rules}\"\n",
        "        return {\"output_data\": document_content, \"feedback\": feedback, \"hold_point\": True, \"prompt_for_user\": \"Review and confirm style rules.\"}\n",
        "\n",
        "class StructuralAnalyzer(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        section_type = \"unknown\"\n",
        "        if section_content.startswith(\"## Requirements\"):\n",
        "            section_type = \"Requirements Section\"\n",
        "        elif section_content.startswith(\"## Glossary\"):\n",
        "            section_type = \"Glossary Section\"\n",
        "        self.context.update({\"current_section_type\": section_type})\n",
        "        feedback = f\"Analyzed section structure: Type - {section_type}\"\n",
        "        return {\"output_data\": section_content, \"feedback\": feedback, \"hold_point\": False}\n",
        "\n",
        "def check_grammar(text, rules=None):\n",
        "    errors = []\n",
        "    if \"shall provides\" in text:\n",
        "        errors.append({\"original\": \"shall provides\", \"suggested\": \"shall provide\", \"explanation\": \"Grammar: verb agreement.\"})\n",
        "    # ... more sophisticated checks based on rules ...\n",
        "    return errors\n",
        "\n",
        "class GrammarEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        edits = []\n",
        "        lines = section_content.splitlines()\n",
        "        style_rules = self.context.get(\"style_rules\", {})\n",
        "        for i, line in enumerate(lines):\n",
        "            grammar_errors = check_grammar(line, style_rules.get(\"grammar_checks\"))\n",
        "            for error in grammar_errors:\n",
        "                edits.append({\"line\": i + 1, **error})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Grammar suggestions.\", \"suggested_edits\": edits}\n",
        "\n",
        "def check_spelling(text, dictionary=None):\n",
        "    errors = []\n",
        "    if \"teh\" in text:\n",
        "        errors.append({\"original\": \"teh\", \"suggested\": \"the\", \"explanation\": \"Spelling error.\"})\n",
        "    # ... more sophisticated checks using dictionary ...\n",
        "    return errors\n",
        "\n",
        "class SpellingEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        edits = []\n",
        "        lines = section_content.splitlines()\n",
        "        # Load dictionary from paths_manager.get_input_path(\"spelling_dict.txt\") (example)\n",
        "        for i, line in enumerate(lines):\n",
        "            spelling_errors = check_spelling(line, self.context.get(\"spelling_dictionary\"))\n",
        "            for error in spelling_errors:\n",
        "                edits.append({\"line\": i + 1, **error})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Spelling suggestions.\", \"suggested_edits\": edits}\n",
        "\n",
        "class ConsistencyEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        edits = []\n",
        "        style_rules = self.context.get(\"style_rules\", {})\n",
        "        terminology = style_rules.get(\"terminology_consistency\", {})\n",
        "        for original, preferred in terminology.items():\n",
        "            if original in section_content:\n",
        "                # Basic replacement - needs more sophisticated handling\n",
        "                section_content = section_content.replace(original, preferred)\n",
        "                edits.append({\"original\": original, \"suggested\": preferred, \"explanation\": f\"Consistent use of '{preferred}'.\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Consistency checks.\", \"suggested_edits\": edits}\n",
        "\n",
        "class ClarityEditor(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        suggestions = []\n",
        "        if \"very important\" in section_content:\n",
        "            suggestions.append({\"line\": section_content.find(\"very important\"), \"suggestion\": \"Consider using a more precise term than 'very important'.\", \"type\": \"Clarity\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Clarity suggestions.\", \"clarity_suggestions\": suggestions}\n",
        "\n",
        "class RequirementValidator(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        issues = []\n",
        "        if self.context.get(\"structured_requirements\"):\n",
        "            for req_id, details in self.context[\"structured_requirements\"].items():\n",
        "                if req_id in section_content and \"measurability\" not in section_content.lower():\n",
        "                    issues.append({\"requirement\": req_id, \"issue\": \"Measurability not explicitly mentioned.\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Requirement validation.\", \"validation_issues\": issues}\n",
        "\n",
        "class ContextReviewer(Agent):\n",
        "    def process(self, section_content: str) -> Dict:\n",
        "        suggestions = []\n",
        "        if self.context.get(\"current_section_type\") == \"Requirements Section\" and \"background information\" in section_content.lower():\n",
        "            suggestions.append({\"suggestion\": \"Consider moving background information to a dedicated section.\", \"type\": \"Structure\"})\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Context review.\", \"context_suggestions\": suggestions}\n",
        "\n",
        "class FeedbackAggregator(Agent):\n",
        "    def process(self, section_content: str, all_agent_outputs: List[Dict]) -> Dict:\n",
        "        aggregated_feedback = {\"edits\": [], \"issues\": [], \"suggestions\": [], \"violations\": []}\n",
        "        for output in all_agent_outputs:\n",
        "            aggregated_feedback[\"edits\"].extend(output.get(\"suggested_edits\", []))\n",
        "            aggregated_feedback[\"issues\"].extend(output.get(\"validation_issues\", []))\n",
        "            aggregated_feedback[\"suggestions\"].extend(output.get(\"clarity_suggestions\", []) + output.get(\"context_suggestions\", []))\n",
        "            aggregated_feedback[\"violations\"].extend(output.get(\"violations\", []))\n",
        "\n",
        "        formatted_output = \"### Aggregated Feedback for Section:\\n\"\n",
        "        if aggregated_feedback[\"edits\"]:\n",
        "            formatted_output += \"**Proposed Edits:**\\n\" + \"\\n\".join([f\"- Line {e['line']}: `{e['original']}` -> `{e['suggested']}` ({e['explanation']})\" for e in aggregated_feedback[\"edits\"]])\n",
        "        if aggregated_feedback[\"issues\"]:\n",
        "            formatted_output += \"\\n**Validation Issues:**\\n\" + \"\\n\".join([f\"- {i['requirement']}: {i['issue']}\" for i in aggregated_feedback[\"issues\"]])\n",
        "        if aggregated_feedback[\"suggestions\"]:\n",
        "            formatted_output += \"\\n**Suggestions:**\\n\" + \"\\n\".join([f\"- {s['type']}: {s['suggestion']}\" for s in aggregated_feedback[\"suggestions\"]])\n",
        "        if aggregated_feedback[\"violations\"]:\n",
        "            formatted_output += \"\\n**Style Violations:**\\n\" + \"\\n\".join([f\"- {v}\" for v in aggregated_feedback[\"violations\"]])\n",
        "\n",
        "        return {\"output_data\": section_content, \"feedback\": formatted_output, \"hold_point\": True, \"prompt_for_user\": \"Review aggregated feedback. Accept/Reject/Revise?\"}\n",
        "\n",
        "class FinalComposer(Agent):\n",
        "    def process(self, full_edited_document: str) -> Dict:\n",
        "        # Basic Markdown formatting\n",
        "        final_md = \"# Edited Document\\n\\n\" + full_edited_document\n",
        "        report = f\"# Editing Report\\n\\nGenerated on: {datetime.datetime.now()}\"\n",
        "        return {\"output_data\": final_md, \"feedback\": \"Final document and report generated.\", \"final_document_md\": final_md, \"editing_report_md\": report, \"hold_point\": True, \"prompt_for_user\": \"Review final document and report.\"}\n",
        "\n",
        "class DiagnosticsAgent(Agent):\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        report = []\n",
        "        if \"TODO:\" in document_content:\n",
        "            report.append(\"Warning: Contains 'TODO:' markers.\")\n",
        "        # Example: Check for consistent heading styles (very basic)\n",
        "        headings = [line for line in document_content.splitlines() if line.startswith(\"#\")]\n",
        "        if headings and not all(h.startswith(\"## \") for h in headings):\n",
        "            report.append(\"Suggestion: Consider consistent use of second-level headings (##).\")\n",
        "        return {\"output_data\": document_content, \"feedback\": \"Diagnostics report:\", \"report\": \"\\n\".join(report), \"hold_point\": True, \"prompt_for_user\": \"Review diagnostics report.\"}\n",
        "\n",
        "class HelpersAgent(Agent):\n",
        "    def extract_glossary(self, document_content: str) -> List[str]:\n",
        "        glossary = [line.split(\":\")[1].strip() for line in document_content.splitlines() if line.startswith(\"Term:\")]\n",
        "        return glossary\n",
        "\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        helpers_output = {}\n",
        "        if \"extract glossary\" in input(\"Available helpers: extract glossary. Which helper? \").lower():\n",
        "            helpers_output[\"glossary\"] = self.extract_glossary(document_content)\n",
        "            feedback = f\"Extracted glossary: {helpers_output['glossary']}\"\n",
        "            hold_point = True\n",
        "            prompt_for_user = \"Review extracted glossary.\"\n",
        "        else:\n",
        "            feedback = \"No helper function invoked.\"\n",
        "            hold_point = False\n",
        "            prompt_for_user = None\n",
        "        return {\"output_data\": document_content, \"feedback\": feedback, \"helpers_output\": helpers_output, \"hold_point\": hold_point, \"prompt_for_user\": prompt_for_user}\n",
        "\n",
        "class FixerAgent(Agent):\n",
        "    def process(self, section_content: str, suggested_edits: List[Dict]) -> Dict:\n",
        "        fixed_content = section_content\n",
        "        applied_fixes = []\n",
        "        for edit in suggested_edits:\n",
        "            if edit[\"explanation\"].startswith(\"Spelling:\") and input(f\"Auto-fix '{edit['original']}' to '{edit['suggested']}'? (y/n)\").lower() == 'y':\n",
        "                fixed_content = fixed_content.replace(edit[\"original\"], edit[\"suggested\"], 1)\n",
        "                applied_fixes.append(edit)\n",
        "        return {\"output_data\": fixed_content, \"feedback\": f\"Applied automatic fixes: {applied_fixes}\", \"applied_fixes\": applied_fixes}\n",
        "\n",
        "class CheckerAgent(Agent):\n",
        "    def process(self, section_content: str, style_rules: Dict) -> Dict:\n",
        "        violations = []\n",
        "        if style_rules.get(\"heading_capitalization\") == \"title case\":\n",
        "            for line in section_content.splitlines():\n",
        "                if line.startswith(\"#\") and line != line.title():\n",
        "                    violations.append(f\"Warning: Heading '{line.strip()}' is not title-cased.\")\n",
        "        return {\"output_data\": section_content, \"feedback\": \"Style checks performed.\", \"violations\": violations, \"hold_point\": True, \"prompt_for_user\": \"Review style check violations.\"}\n",
        "\n",
        "class AnalyzerAgent(Agent):\n",
        "    def process(self, document_content: str) -> Dict:\n",
        "        readability = len(document_content.split()) / (document_content.count('.') + 1 if document_content.count('.') > 0 else 1) # Very basic\n",
        "        return {\"output_data\": document_content, \"feedback\": f\"Read"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "PjyTjE8fTmGq"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}